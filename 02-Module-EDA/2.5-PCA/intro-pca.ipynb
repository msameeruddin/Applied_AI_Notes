{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA (Principal Component Analysis)\n",
    "\n",
    "> ### [Helful Link](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We use it for dimensionality reduction.\n",
    "* $d -$ dimensions are reduced to $d^1$ where $d^1 < d$.\n",
    "* $d -$ dimensional data can be visualized to `2D` easily using PCA.\n",
    "* Simplest and fundamental technique used in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Spreas is a measure of information. In PCA, we try to preserve those which has high variability and discard the one which has less.\n",
    "* More spread means more information.\n",
    "\n",
    "![geometric-pca](https://user-images.githubusercontent.com/63333753/113476593-0d10a080-949a-11eb-959b-705776997034.png)\n",
    "\n",
    "* From the figure, $g_1$ is the rotated axis of $f_1$ and so is $g_2$ of $f_2$.\n",
    "* By observing the rotated axes, we can say that the variability in $g_2$ is very less and thus can be discarded.\n",
    "* We preserve $f_1$ (by discarding $f_2$).\n",
    "\n",
    "---\n",
    "\n",
    "* PCA name itself signifies what it does. It identifies the main features which can distinguish between the classes.\n",
    "\n",
    "* Before going for PCA, we should do `column standardization` (CS). CS can help us bring the features under unifom metric and we can observe the variance of each feature easily.\n",
    "\n",
    "* From Geometrical interpretaion, if we are unable to eliminate one feature from scattered plate by analysing Variance, we can rotate the axis of features by some angle and align the rotated line on to the data points. This way we can analyse variance more lucidly and eliminate one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Objective (Max Variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming the data is column standardized, from above we can further derive -\n",
    "\n",
    "* Let's call $g_1$ as $u_1$, and the main important thing we should be concerned is finding the direction of $u_1$ (once we know the direction, we can easily find the axis).\n",
    "* Typically, we represent the direction in the form of unit vector such that $||u_1|| = 1$\n",
    "\n",
    "![unit_vector_axis](https://user-images.githubusercontent.com/63333753/113499747-fd469a00-9535-11eb-8ab7-313edfb1696e.png)\n",
    "\n",
    "* $x_j = proj_{u_1} x_i = \\frac{u_1.x_i}{||u_1|| (=1)} = u_1^Tx_i \\rightarrow (i)$\n",
    "    - Thus $x_j = u_1^Tx_i$\n",
    "    - Let's call $x_j$ as $x_i^1$ which implies $x_i^1 = u_1^Tx_i \\rightarrow (1)$\n",
    "* Let $D = \\{x_i\\}_{i=1}^n$ and $D^1 = \\{x_i^1\\}_{i=1}^n$\n",
    "* Given $\\bar{x}$ (the mean of $x_i$), we can find the mean of $x_i^1$ -\n",
    "    - From $(1)$, we can write, $\\bar{x^1} = u_1^T\\bar{x} \\rightarrow (2)$\n",
    "* Our main aim is to find $u_1$ such that $\\text{Var} \\{proj_{u_1} x_i\\}_{i=1}^n$ is maximal\n",
    "    - From $(1)$, we can write $$\\text{Var} \\{x_i^1\\}_{i=1}^n = \\frac{1}{n} \\sum_{i=1}^n \\big(u_1^Tx_i - u_1^T\\bar{x}\\big)^2 \\rightarrow (3)$$\n",
    "    - Since the data is column standardized, we have our $\\bar{x} = [0, 0, 0, \\dots, 0]$. If $\\bar{x} is 0$ then $u_1^T\\bar{x} is 0$ which simply results in $$\\text{Var} \\{x_i^1\\}_{i=1}^n = \\frac{1}{n} \\sum_{i=1}^n \\big(u_1^Tx_i)^2 \\rightarrow (4)$$\n",
    "    - We want to maximize the variance in order to preserve the feature and discard the one of less variance. And we need to find $u_1$ such that $(4)$ is maximized only by taking the unit vector as 1 not other than that ($u_1^Tu_1 = 1$). This is called optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Minimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![distance_minimize](https://user-images.githubusercontent.com/63333753/113500835-da6cb380-953e-11eb-93e8-c30f5b05a739.png)\n",
    "\n",
    "* We know that $proj_{u_1} x_i = u_1^Tx_i$\n",
    "* From trignometry, we can say $$d_i^2 = ||x_i||^2 - (u_1^Tx_i)^2$$ which implies $$d_i^2 = x_i^Tx_i - (u_1^Tx_i)^2$$\n",
    "* Our main aim is to minimize the $d_i^2$ such that $u_1$ is a unit vector ($u_1^Tu_1 = 1$). This is called distance minimizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen Values and Eigen Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eigen values are often represented as $(\\lambda_1, \\lambda_2, \\dots, \\lambda_d)$.\n",
    "* Eigen vectors are often represented as $(V_1, V_2, \\dots, V_d)$.\n",
    "---\n",
    "* We know that the co-variance matrix $S = X^TX$ which is $d \\text{x} d$ matrix.\n",
    "* If we compute the eigen values of $S_{d\\text{x}d}$, we will have eigen values as $(\\lambda_1, \\lambda_2, \\dots, \\lambda_d)$.\n",
    "* For every eigen value thre is a corresponding eigen vector such as $(V_1, V_2, \\dots, V_d)$ where - $$\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\dots \\geq \\lambda_d$$ here, $\\lambda_1$ is the maximum of all.\n",
    "\n",
    "**Definition** - If there is a scalar ($\\lambda_1$) and a vector ($V_1$) which is of ${d \\text{x} 1}$ dimension and a co-variance matrix ($S_{d\\text{x}d}$), and if we have a relationship like - $$\\lambda_1V_1 = SV_1$$ then\n",
    "\n",
    "* $\\lambda_1 \\rightarrow$ is the eigen value of $S$.\n",
    "* $V_1 \\rightarrow$ is the eigen vector to $S$ corresponding to $\\lambda_1$.\n",
    "\n",
    "**Note**\n",
    "\n",
    "* No two eigen vectors are parallel (in the sense, they are always to perpendicular to each other).\n",
    "    - Eigen vector tells the maximal variance in decreasing order like - $$V_1 \\geq V_2 \\geq V_3 \\geq \\dots \\geq V_d$$\n",
    "* On the other hand, eigen values gives the exact percentage of variance in the data matrix take features.\n",
    "    - The condition is like - $$\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\dots \\geq \\lambda_d$$\n",
    "    - $\\frac{\\lambda_i}{\\sum_{i=1}^n \\lambda_i} \\rightarrow$ formula to compute percentage variance (of course, we should know the $\\lambda's$ to do so).\n",
    "    - Tells if the data is spread only on one axis or more axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Principal Component Analysis (PCA) is a statistical techniques used to reduce the dimensionality of the data (reduce the number of features in the dataset) by selecting the most important features that capture maximum information about the dataset.\n",
    "\n",
    "* The features are selected on the basis of variance that they cause in the output. Original features of the dataset are converted to the Principal Components which are the linear combinations of the existing features. The feature that causes highest variance is the first Principal Component. The feature that is responsible for second highest variance is considered the second Principal Component, and so on.\n",
    "\n",
    "> In simple words, Principal Component Analysis is a method of extracting important features (in the form of components) from a large set of features available in a dataset.\n",
    "\n",
    "* PCA finds the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information. By projecting our data into a smaller space, weâ€™re reducing the dimensionality of our feature space.\n",
    "\n",
    "**Advantages of Principal Component Analysis**\n",
    "\n",
    "1. Removes Correlated Features: In a real world scenario, this is very common that you get thousands of features in your dataset. You cannot run your algorithm on all the features as it will reduce the performance of your algorithm and it will not be easy to visualize that many features in any kind of graph. So, you MUST reduce the number of features in your dataset.\n",
    "\n",
    "> You need to find out the correlation among the features (correlated variables). Finding correlation manually in thousands of features is nearly impossible, frustrating and time-consuming. PCA does this for you efficiently.\n",
    "\n",
    "> After implementing the PCA on your dataset, all the Principal Components are independent of one another. There is no correlation among them.\n",
    "\n",
    "2. Improves Algorithm Performance: With so many features, the performance of your algorithm will drastically degrade. PCA is a very common way to speed up your Machine Learning algorithm by getting rid of correlated variables which don't contribute in any decision making. The training time of the algorithms reduces significantly with less number of features.\n",
    "\n",
    "> So, if the input dimensions are too high, then using PCA to speed up the algorithm is a reasonable choice.\n",
    "\n",
    "3. `Improves Visualization`: It is very hard to visualize and understand the data in high dimensions. PCA transforms a high dimensional data to low dimensional data (2 dimension) so that it can be visualized easily.\n",
    "\n",
    "4. We can use 2D Scree Plot to see which Principal Components result in high variance and have more impact as compared to other Principal Components.\n",
    "\n",
    "5. Even the simplest IRIS dataset is 4 dimensional which is hard to visualize. We can use PCA to reduce it to 2 dimension for better visualization.\n",
    "\n",
    "6. Consider a situation where we have 50 features (p = 50). There can be p(p-1)/2 scatter plots i.e. 1225 plots possible to analyze the variable relationships. It would be a tedious job to perform exploratory analysis on this data. That is why, we have to use PCA to get rid of this problem.\n",
    "\n",
    "**Disadvantages of Principal Component Analysis**\n",
    "\n",
    "1. Independent variables become less interpretable: After implementing PCA on the dataset, your original features will turn into Principal Components. Principal Components are the linear combination of your original features. Principal Components are not as readable and interpretable as original features.\n",
    "\n",
    "2. Data standardization is must before PCA: You must standardize your data before implementing PCA, otherwise PCA will not be able to find the optimal Principal Components.\n",
    "\n",
    "> For instance, if a feature set has data expressed in units of Kilograms, Light years, or Millions, the variance scale is huge in the training set. If PCA is applied on such a feature set, the resultant loadings for features with high variance will also be large. Hence, principal components will be biased towards features with high variance, leading to false results.\n",
    "\n",
    "> Also, for standardization, all the categorical features are required to be converted into numerical features before PCA can be applied.\n",
    "\n",
    "> PCA is affected by scale, so you need to scale the features in your data before applying PCA. Use StandardScaler from Scikit Learn to standardize the dataset features onto unit scale (mean = 0 and standard deviation = 1) which is a requirement for the optimal performance of many Machine Learning algorithms.\n",
    "\n",
    "3. `Information Loss`: Although Principal Components try to cover maximum variance among the features in a dataset, if we don't select the number of Principal Components with care, it may miss some information as compared to the original list of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
