{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>T-distributed Stochastic Neighbourhood Embedding</h1></center>\n",
    "\n",
    "* Refer to\n",
    "    - https://distill.pub/2016/misread-tsne/\n",
    "    \n",
    "    - https://colah.github.io/posts/2014-10-Visualizing-MNIST/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* State of the art dimensionality reduction technique for visualization.\n",
    "* Tries at high extent to preserve the neighbourhood.\n",
    "* Neighbourhood preserving embedding.\n",
    "* Probabilistic Algorithm not a Deterministic one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neighbourhood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The set of points whose distance from a given point is less than (or less than or equal to) some value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An embedding is a relatively low-dimensional space into which you can translate high-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Geometrically, we can represent the working t-sne method in the following image -\n",
    "\n",
    "![geometric-intuition-tsne](https://user-images.githubusercontent.com/63333753/113669991-29971d80-96d2-11eb-8749-d394e6d747fb.png)\n",
    "\n",
    "* In `d-dimensions` category, $\\{x_2, x_3\\}$ are the neighbourhoods of $x_1$. Therefore the $d(x_1, x_2)$ and $d(x_1, x_3)$ are very minimal. Similarly, $\\{x_5\\}$ is the neighbourhood of $x_5$ and $d(x_4, x_5)$ is also minimal. But $d(x_1, x_4)$ and $d(x_1, x_5)$ is maximual and therefore they cannot be neighbourhoods of $x_1$.\n",
    "\n",
    "* On the otherhand, embeddings helps to translate the high dimension data to low dimension which is one of the techniques implemented by T-SNE. Therefore, T-SNE makes sure that it retails sufficient information while doing so. The placing of the points in `2-dimensions` may differ but it preserves the neighbourhoods as visulized in the `d-dimensions` part.\n",
    "    - $d(x_1, x_2) \\approx d(g_1, g_2)$\n",
    "    - $d(x_1, x_3) \\approx d(g_1, g_3)$\n",
    "    - $d(x_4, x_5) \\approx d(g_4, g_5)$\n",
    "    - $d(x_1, x_4) \\neq d(g_1, g_4)$\n",
    "    - $d(x_1, x_5) \\neq d(g_1, g_5)$\n",
    "* T-SNE tries at high extent to preserve the neighbourhood (Neighbourhood preserving embedding).\n",
    "\n",
    "**Note** - $d(p_1, p_2)$ is the geometric distance between $p_1$ and $p_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sometimes, it is impossible to preserve distances in all the neighbourhoods. This case is called as **Crowding Problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* With T-SNE\n",
    "    - we cannot interpret cluster sizes\n",
    "    - we cannot interpret the inter-cluster distances\n",
    "* Only groups the data based on the visual similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "> Also refer to - [Analytics Vidya Blog Post](https://www.analyticsvidhya.com/blog/2017/03/questions-dimensionality-reduction-data-scientist/?utm_medium=social&utm_source=linkedin.com&utm_campaign=buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is Dimensionality Reduction?\n",
    "    - Dimensionality reduction means projecting data to a lower-dimensional space, which makes it easier for the visualization and analysis of data.\n",
    "\n",
    "2. Explain Principal Component Analysis?\n",
    "    - PCA means finding out the components (features) which are effective to the data and discarding the redundant features.\n",
    "\n",
    "3. Importance of PCA?\n",
    "    - With few lines of codes we can reduce the dimensions by a huge number.\n",
    "\n",
    "4. Limitations of PCA?\n",
    "    - PCA does preserve the global direction of the data but not the local, which creates confusion when an overlap of 2 clusters happens after the reduction.\n",
    "\n",
    "5. What is t-SNE?\n",
    "    - t-SNE stands for t-distribution Scholastic Neighbourhood Embedding.\n",
    "    - Scholastic – not definite but random probability\n",
    "    - Neighborhood – concerned only about retaining the structure of neighborhood points.\n",
    "    - Embedding – plotting data into lower dimensions tSNE is the state of the art or one of the best techniques for dimensionality reduction, which is widely used for data visualization.\n",
    "\n",
    "6. What is Crowding problem?\n",
    "    - When a data point, ‘x’ is a neighbor to 2 data points that are not neighboring to each other, this may result in losing the neighborhood of ‘x' with one of the data points as t-SNE is concerned only within the neighborhood zone.\n",
    "\n",
    "7. How to apply t-SNE and interpret its output?\n",
    "    - There are 3 parameters\n",
    "        - Steps: number of iterations\n",
    "        - Perplexity: can be thought of as the number of neighboring points.\n",
    "        - Epsilon: It is for data visualization and determines the speed which it should be changed.\n",
    "\n",
    "\n",
    "\n",
    "**Points to remember while performing tSNE**\n",
    "\n",
    "1. Never stop with a single-step value. Check for various values and take the value at which the plot is stable.\n",
    "\n",
    "2. With lower perplexity values, we may see a few shapes of clusters. But do not fall into the trap. Try with various Perplexity values ranging from 2 to the number of data points. But, remember a value of 2 or a value equal to a number of data points will lead to no information.\n",
    "\n",
    "3. Never come to any conclusions with random data.\n",
    "\n",
    "4. As tSNE is Scholastic, each run may lead to slightly different. However, by setting random_state, this can be solved.\n",
    "\n",
    "5. tSNE doesn’t preserve the distance between clusters. So, when we have multiple clusters, we might not retain the similar distance between the clusters.\n",
    "\n",
    "6. tSNE shrinks the widespread data and expands densely packed data. So based on the output, we cannot decide on the cluster size and density/spread /variance of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
