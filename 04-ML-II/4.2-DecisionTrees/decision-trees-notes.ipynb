{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Similar to classification algorithms like \n",
    "    - K-NN\n",
    "    - Naive Bayes\n",
    "    - Logistic Regression\n",
    "    - Linear Regressioin\n",
    "    - SVM\n",
    "    we have another algorithm popularly known as **Decision Trees**.\n",
    "\n",
    "* Decision tree algorithm is simply a nested `if-else` condition classifier.\n",
    "\n",
    "* Decision tree models are highly interpretable.\n",
    "\n",
    "* A sample code can represented as a decision tree.\n",
    "\n",
    "![dt-1](https://user-images.githubusercontent.com/63333753/123220619-c2785080-d4eb-11eb-9093-f23d5ffc706f.png)\n",
    "\n",
    "* A decision tree can be visualized geometrically.\n",
    "\n",
    "![dt-2](https://user-images.githubusercontent.com/63333753/123220683-cc9a4f00-d4eb-11eb-8fed-b4261c4ba0bd.png)\n",
    "\n",
    "> In DT, all the hyperplanes are axis parallel and intuitively, it is a set of axis parallel hyperplanes.\n",
    "\n",
    "**Terminology**\n",
    "\n",
    "* Root node → The very first node in a tree.\n",
    "* Leaf nodes' → The terminating nodes in a tree.\n",
    "* Internal nodes → The nodes which are neither leaf nodes or root nodes in a tree.\n",
    "\n",
    "> At all non-leaf nodes, we have a decision/condition in a tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Decision Tree - Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The toughest task is to build a decision tree given the training data set.\n",
    "\n",
    "* Decision trees can be built by applying the concept of entropy (the concept of entropy is used in information theory).\n",
    "\n",
    "* Suppose we are given a random variable $Y$ which can take $k$ values.\n",
    "\n",
    "$$Y = \\{y_1, y_2, y_3, \\dots, y_k \\}$$\n",
    "\n",
    "* Entropy of $Y$ is defined as $H(Y)$.\n",
    "\n",
    "$$H(Y) = - \\sum_{i=1}^k P(y_i) \\log_b[P(y_i)]; \\text{where} \\ (b = 2 \\ \\text{or} \\ 2.718)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_source = 'http://www.shatterline.com/MachineLearning/data/tennis_anyone.csv'\n",
    "df = pd.read_csv(data_source)\n",
    "df.columns = ['outlook', 'temp', 'humidity', 'wind', 'class']\n",
    "\n",
    "def compute_entropy(dframe, f):\n",
    "    udf = dframe[f].value_counts().to_frame()\n",
    "    unique_vals = udf.index.to_list()\n",
    "    fsum = np.sum(udf[f].to_list())\n",
    "    \n",
    "    fh = {}\n",
    "    for i in unique_vals:\n",
    "        icount = len(dframe[dframe[f] == i])\n",
    "        iprob = icount / fsum\n",
    "        iprob_log = iprob * np.log2(iprob)\n",
    "        fh[i] = iprob_log\n",
    "    \n",
    "    entropy = np.sum(list(fh.values()))\n",
    "    return round(-entropy, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook → 1.577\n",
      "temp → 1.557\n",
      "humidity → 1.0\n",
      "wind → 0.985\n",
      "class → 0.94\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    ent_ = compute_entropy(dframe=df, f=i)\n",
    "    print(\"{} → {}\".format(i, ent_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $Y$ be a random variable which can take $2$ values $\\rightarrow y_+, y_-$\n",
    "\n",
    "* **Case 1**\n",
    "    - $y_+ \\rightarrow 99%$\n",
    "    - $y_- \\rightarrow 1%$\n",
    "    - $H(Y) \\rightarrow 0.0801$\n",
    "\n",
    "* **Case 2**\n",
    "    - $y_+ \\rightarrow 50%$\n",
    "    - $y_- \\rightarrow 50%$\n",
    "    - $H(Y) \\rightarrow 1$\n",
    "\n",
    "* **Case 3**\n",
    "    - $y_+ \\rightarrow 0%$\n",
    "    - $y_- \\rightarrow 100%$\n",
    "    - $H(Y) \\rightarrow 0$\n",
    "\n",
    "**Entropy Plot for Binary Class**\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/c/c9/Binary_entropy_plot.png\">\n",
    "\n",
    "> The more peaked a distribution is the smaller is its entropy, and vice-versa.\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL-Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given pdf of $P$ and pdf $Q$ such that $P$ and $Q$ be any two distributions of random variable $X$.\n",
    "\n",
    "* The distance between $P$ and $Q$ is referred to as KL-Divergence.\n",
    "\n",
    "* Using the concept of KS-Statistic, we can compute the distance between $P$ and $Q$ by considering the cdf of $P$ and $Q$.\n",
    "\n",
    "* Here, we take the maximum distance by which cdf of $P$ and cdf $Q$ are separated.\n",
    "\n",
    "* The only problem with using KS-Statistic is that it is not differentiable. In most of the machine learning concepts, differentiation plays a major role.\n",
    "\n",
    "* With the help of KL-Divergence, we can get the distance that can be differentiated.\n",
    "\n",
    "$$D_{KL}(P||Q) = \\sum_x P(x) \\log_2 \\bigg[\\frac{P(x)}{Q(x)}\\bigg] \\ \\text{or} \\ \\int_x P(x) \\log_2 \\bigg[\\frac{P(x)}{Q(x)}\\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Decision Tree - Gini Impurity ($I_G$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gini impurity is very similar to entropy.\n",
    "* Given $Y = \\{y_1, y_2, y_3, \\dots, y_k\\}$, gini impurity is defined as -\n",
    "\n",
    "$$I_G(Y) = 1 - \\sum_{i=1}^k \\big[P(y_i)\\big]^2$$\n",
    "\n",
    "* Let $Y$ be a random variable which can take $2$ values $\\rightarrow y_+, y_-$\n",
    "\n",
    "    * **Case 1**\n",
    "        - $y_+ \\rightarrow 50%$\n",
    "        - $y_- \\rightarrow 50%$\n",
    "        - $I_G(Y) \\rightarrow 0.5$\n",
    "\n",
    "    * **Case 2**\n",
    "        - $y_+ \\rightarrow 0%$\n",
    "        - $y_- \\rightarrow 100%$\n",
    "        - $H(Y) \\rightarrow 0$\n",
    "\n",
    "* Gini impurity is more computationally efficient than entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_source = 'http://www.shatterline.com/MachineLearning/data/tennis_anyone.csv'\n",
    "df = pd.read_csv(data_source)\n",
    "df.columns = ['outlook', 'temp', 'humidity', 'wind', 'class']\n",
    "\n",
    "def compute_gini_impurity(dframe, f):\n",
    "    udf = dframe[f].value_counts().to_frame()\n",
    "    unique_vals = udf.index.to_list()\n",
    "    fsum = np.sum(udf[f].to_list())\n",
    "    \n",
    "    fh = {}\n",
    "    for i in unique_vals:\n",
    "        icount = len(dframe[dframe[f] == i])\n",
    "        iprob = (icount / fsum) ** 2\n",
    "        fh[i] = iprob\n",
    "    \n",
    "    gm = 1 - np.sum(list(fh.values()))\n",
    "    return round(gm, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.663\n",
      "0.653\n",
      "0.5\n",
      "0.49\n",
      "0.459\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    gm = compute_gini_impurity(dframe=df, f=i)\n",
    "    print(gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Decision Tree - Information Gain (IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Information Gain = [Entropy(parent)] – [Weighted Average Entropy of child nodes]}$$\n",
    "\n",
    "$$\\text{or}$$\n",
    "\n",
    "$$\\text{IG(Y, var)} = H_D(Y) - \\sum_{i=1}^k \\frac{|D_i|}{|D|} H_{D_i}(Y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data_source = 'http://www.shatterline.com/MachineLearning/data/tennis_anyone.csv'\n",
    "df = pd.read_csv(data_source)\n",
    "df.columns = ['outlook', 'temp', 'humidity', 'wind', 'class']\n",
    "\n",
    "def compute_weighted_entropy(dframe, f, c):\n",
    "    fudf = dframe[f].value_counts().to_frame()\n",
    "    funique_vals = fudf.index.to_list()\n",
    "    c_tot = len(dframe[c])\n",
    "    \n",
    "    finfo = {}\n",
    "    for i in funique_vals:\n",
    "        fidf = dframe[dframe[f] == i]\n",
    "        fent_ = compute_entropy(dframe=fidf, f=c)\n",
    "        finfo[i] = (len(fidf) / c_tot) * fent_\n",
    "    \n",
    "    went_ = np.sum(list(finfo.values()))\n",
    "    return round(went_, 3)\n",
    "\n",
    "def gain_information(dframe, f, c):\n",
    "    went_ = compute_weighted_entropy(dframe=dframe, f=f, c=c)\n",
    "    pent_ = compute_entropy(dframe=df, f=c)\n",
    "    ginfo = pent_ - went_\n",
    "    return round(ginfo, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.694"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_weighted_entropy(dframe=df, f='outlook', c='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlook → 0.246\n",
      "temp → 0.029\n",
      "humidity → 0.152\n",
      "wind → 0.048\n"
     ]
    }
   ],
   "source": [
    "for i in df.columns:\n",
    "    if (i == 'class'):\n",
    "        break\n",
    "    ig = gain_information(dframe=df, f=i, c='class')\n",
    "    print(\"{} → {}\".format(i, ig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a Decision Tree\n",
    "\n",
    "Helpful link → https://bit.ly/3d9noYg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pure node** → The node which has only one class label is called a pure node.\n",
    "\n",
    "1. Choose the root node by computing information gain on all the features w.r.t target and pick the maximum.\n",
    "\n",
    "2. Split the data by likeliness\n",
    "    - If any of the nodes is pure node, do not extend further.\n",
    "    - Otherwise, continue.\n",
    "\n",
    "3. Choose the internal node by step 1\n",
    "\n",
    "> This entire process is computed recursively. IG plays a major role in visualizing the decision tree.\n",
    "\n",
    "**Rules**\n",
    "\n",
    "1. If pure node, stop growing the tree.\n",
    "\n",
    "2. If lack of points, growing the tree is impossible.\n",
    "\n",
    "3. If the depth of a tree is too large, stop growing the tree.\n",
    "    - As the depth increases, chances of overfitting to noise increases.\n",
    "    - If depth is very small, the model underfits\n",
    "    - The hyperparameter is `depth` and the right `depth` is decided by cross-validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Decision Tree - Splitting Numerical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Splitting in numerical data has to be done by -\n",
    "\n",
    "    - Sort the numerical features in ascending order.\n",
    "    - Conditional for splitting can be decided in `n` possible ways, like -\n",
    "        * f1 < thresh_1\n",
    "        * f2 < thresh_2\n",
    "        * f3 < thresh_3\n",
    "        * ...\n",
    "        * fn < thresh_n\n",
    "        * We need to evaluate for all `n` conditions.\n",
    "    - Pick the one that gives maximum IG.\n",
    "\n",
    "* This complete process is very time consuming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Decision Tree - Categorical Features with `n` Possible Ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In case of the categorical data with `n` possible ways, it is better to convert the data into numeric by considering the feature along with the target variable and compute the conditional probability.\n",
    "\n",
    "* Replace the categorical value with the probability and construct a decision tree.\n",
    "\n",
    "When we have a categorical feature with many elements and it is nominal in nature then we have three approaches to converting this feature.\n",
    "\n",
    "* CASE1 (when response variable is both categorical/continues):\n",
    "    - We can bin the feature into fewer subcategories.\n",
    "\n",
    "* CASE2 (when response variable is continues):\n",
    "    - Replace each category with its mean/median response variable value.\n",
    "\n",
    "* CASE3 (when response variable is discrete/categorical):\n",
    "    - Replace each x_ij with P(y_i=C|F_j=\"ABC\")\n",
    "\n",
    "CASE1 is known as binning\n",
    "\n",
    "CASE2 and CASE3 is known as response variable encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A tree of depth 1 is called a decision stump.\n",
    "\n",
    "* If depth of the tree increases, \n",
    "    - the possibility of having very few points at leaf node increases.\n",
    "    - model gets overfitted to the noisy data.\n",
    "    - interpretability of the model decreases and this should never happen.\n",
    "\n",
    "* If depth of the tree is shallow,\n",
    "    - model get underfitted to the data.\n",
    "\n",
    "> The right depth needs to be discovered by cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Runtime Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training time complexity** → $O[n\\log_2(n)d]$\n",
    "    - `n` → number of points in the data\n",
    "    - `d` → dimensionality of the data\n",
    "    - $n\\log_2(n)$ → corresponds to sorting\n",
    "* **Space complexity** → after training the data, we need to convert the tree into `if-else` condition. The space complexity is \n",
    "    - $(\\text{total number of internal nodes}) + (\\text{total number of leaf nodes})$\n",
    "    - or\n",
    "    - $(\\text{total number of nodes}$)\n",
    "* **Runtime complexity** → it is just the order of depth of the tree; $O(\\text{depth})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regression\n",
    "\n",
    "Helpful link → http://www.saedsayad.com/decision_tree_reg.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If imbalance data, balance it and do the process.\n",
    "* When the data has categorical features, avoid one-hot encoding. Otherwise, the model take much time to get trained.\n",
    "* Instead of data, if given similarity matrix, decision trees cannot work. Decision trees need features for IG.\n",
    "* Feature interaction is used to take a decision for a query point to belong to anuy of the class labels.\n",
    "    - Logical feature interactions are in-built in decision trees.\n",
    "* As depth increases, the impact of outliers is more.\n",
    "* Interpretability of the features is very easy as everything can be changed to `if-else` conditions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
