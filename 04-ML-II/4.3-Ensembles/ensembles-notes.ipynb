{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Meaning → Group of things.\n",
    "* In the context of machine learning it is simply the combination multiple models so as to build a powerful model.\n",
    "* Simple models → $[m_1, m_2, m_3, \\dots, m_k]$ → also called as base models. We combine these base models to contruct a powerful model.\n",
    "\n",
    "**4 types of ensembles**\n",
    "\n",
    "* Bagging (Bootstraped aggregation)\n",
    "* Boosting\n",
    "* Stacking\n",
    "* Cascading\n",
    "\n",
    "(Using these techniques, we can build a very high-performing model)\n",
    "\n",
    "> The more different the base models are, the better we can combine them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "\n",
    "<p><span style=\"color:red\">B</span>ootstraped <span style=\"color:red\">agg</span>regation → <span style=\"color:red\">Bagging</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RandomForest algorithm extensively uses bagging techniques.\n",
    "\n",
    "* Divide the training data ($D_n$) into `k` samples of size `m` (sampling is done with replacement).\n",
    "\n",
    "* Apply `k` models on each sample correspondingly.\n",
    "    - Each model $m_i$ is built on each sample $D_n^i$ correspondingly.\n",
    "    - Each model $m_i$ sees a different sample of data.\n",
    "\n",
    "* Aggregate the models (combine) into a powerful model $M$. Typically this is called an aggregation stage.\n",
    "\n",
    "![ensemble-1](https://user-images.githubusercontent.com/63333753/125157308-e2647100-e187-11eb-99e0-a9bf6fa4f755.png)\n",
    "\n",
    "> If we are doing a classifiction task, the aggregation would be majority vote techinique. <br> If we are doing a regression task, the aggregation would be mean or median.\n",
    "\n",
    "* For the prediction, we send the query point to each model $m_i$ and get the each prediction. From that set of predictions, we apply majority voting technique and get the actual prediction. (This is only classification task).\n",
    "    - For regression, we simple compute the average or median of predictions from each model and get the actual prediction.\n",
    "\n",
    "**Note**\n",
    "\n",
    "* Variance → siginifies how much a model changes for a slight change in the data. If a model changes a lot, then it has high variance.\n",
    "\n",
    "* Because, we apply sampling, the overall result doesn't change much though the data is changed a lot. This is one advantage of Bagging.\n",
    "\n",
    "> Bagging can reduce variance in the model without impacting the bias.\n",
    "\n",
    "$$\\text{Model Error} = \\text{Bias}^2 + \\text{Variance}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of RandomForest (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The name comes from decision trees algorithms. A bunch of decision trees (or different models) with data being randomly sampled is called a RandomForest model. Bagging technique is used in RF.\n",
    "\n",
    "$$\\text{RF} = \\text{DT} + \\text{Bagging} + \\text{Column Sampling}$$\n",
    "\n",
    "* Column sampling is done without replacement, whereas bagging is done (row sampling) with replacement. Thus, the models are going to be different since the features will be different.\n",
    "\n",
    "![ensemble-2](https://user-images.githubusercontent.com/63333753/125159903-d4b6e780-e197-11eb-8895-0619d3a48469.png)\n",
    "\n",
    "All the models (DTs) are trained to the fullest depth having high variance and less bias. All the points which are remaining after selection of sample are called `Out of Bag` points which are used for cross validating the model. This process is default in RF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RF → low bias because the base model start-off with having less bias.\n",
    "    - $\\text{Bias}(M) \\equiv \\text{Bias}(m_i)$\n",
    "\n",
    "$$M = \\text{Agg}(m_1, m_2, m_3, \\dots, m_k)$$\n",
    "\n",
    "* As $k$ ↑; Variance ↓\n",
    "* As $k$ ↓; Variance ↑"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging - Training & Runtime Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* RF with `k` base-learners (base-models) $\\implies$ (DTs)\n",
    "    - **Training** → $O(n\\log(n) * k)$\n",
    "        - Since we have samples, we can bunch of samples parallely (this is known as Trivially Parallelizable).\n",
    "    - **Runtime** → $O(\\text{depth} * k)$\n",
    "    - **Space** → $O(\\text{DT} * k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForest Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DT doesn't work well in the cases where the data features large categorical data.\n",
    "* Wherever DT fails, RF also fails.\n",
    "* In the case od DT, bias depends on the depth of tree which is not same as RF. In RF, the bias depends on `k` models and in each model the depth is reasonable.\n",
    "* In DT, the feature importance depends on the weighted sum of overall reduction of entropy or IG because of certain features at various levels in DT.\n",
    "    - If a feature contributes to reducing the entropy or IG more often and at more points, then that feature is considered to be important.\n",
    "    - The same concept is used for RF in determing the feature importance but of course, for `k` models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Boosting} = \\text{Base learners (high bias & low variance)} + \\text{Additive Combining}$$\n",
    "\n",
    "* There is no row sampling or col sampling and aggregation. Instead of this, we have additive combination.\n",
    "\n",
    "* The ultimate purpose is to reduce the bias while keeping the variance low.\n",
    "\n",
    "* (High bias indicates a tree which is very shallow or not deep).\n",
    "\n",
    "**Process Steps**\n",
    "\n",
    "1. **0th stage**\n",
    "    - Train the model $m_0$ on the whole training data $\\{x_i, y_i\\}$.\n",
    "    - $y = h_0(X) \\rightarrow$ base model\n",
    "    - Compute simple difference error $\\implies e_i^{\\text{stage 0}} = y_i - h_0(x_i)$\n",
    "    - For each point in the training data we have $\\{x_i, e_i^{\\text{stage 0}}, y_i\\}_{i=1}^n$\n",
    "\n",
    "2. **1st stage**\n",
    "    - Train the model $m_1$ on the whole training data. Instead of predicting $y_i$, predict the $e_i^{\\text{stage 0}}$.\n",
    "    - $e_i^{\\text{stage 0}} = y_i - h_0(x_i)$\n",
    "    - $h_1(X) \\rightarrow$ base model\n",
    "    - Main model at the end of the first stage is $F_1(X) = \\alpha_0 h_0(X) + \\alpha_1 h_1(X)$. We shall find the values of $\\alpha_0$ and $\\alpha_1$.\n",
    "    - $e_i^{\\text{stage 1}} = y_i - F_1(x_i)$\n",
    "\n",
    "3. **3rd stage**\n",
    "    - Train the model $m_2$ on the whole training data. Instead of predicting $y_i$, predict the $e_i^{\\text{stage 0}}$.\n",
    "    - $e_i^{\\text{stage 1}} = y_i - F_1(x_i)$\n",
    "    - $h_2(X) \\rightarrow$ base model\n",
    "    - Main model at the end of the second stage is $F_2(X) = \\alpha_0 h_0(X) + \\alpha_1 h_1(X) + \\alpha_2 h_2(X)$. We shall find the values of $\\alpha_0, \\alpha_1$, and $\\alpha_2$.\n",
    "    - $e_i^{\\text{stage 2}} = y_i - F_2(x_i)$\n",
    "\n",
    "4. **...**\n",
    "    - Main model at the end of `kth` stage is $F_k(X) = \\sum_{i=0}^k \\alpha_i h_i(X) \\implies$ additive combination.\n",
    "    - $e_i^{\\text{stage k - 1}} = y_i - F_{k - 1}(x_i)$\n",
    "\n",
    "Thus, $F_k(X)$ ends up having a low residual error which simply means low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residuals, Loss Functions, and Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$F_k(x) = \\sum_{i=0}^k \\alpha_i h_i(x)$$\n",
    "\n",
    "$$\\text{Residuals at the end of the stage k} \\implies e_i = y_i - F_k(x_i)$$\n",
    "\n",
    "* In linear regression we mainly try to reduce the squared loss. If we think in the perspective linear regression then -\n",
    "\n",
    "$$L[y_i, F_k(x_i)] = [y_i - F_k(x_i)]^2 \\rightarrow (1)$$\n",
    "\n",
    "* If we differentiate $(1)$ w.r.t $F_k(x_i)$, we get\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial F_k(x_i)} = -2 [y_i - F_k(x_i)]$$\n",
    "\n",
    "$$-\\frac{\\partial L}{\\partial F_k(x_i)} = 2 [y_i - F_k(x_i)]$$\n",
    "\n",
    "$$\\text{Negative Derivative} = 2 (\\text{Residual})$$\n",
    "\n",
    "* Negative gradient or derivative can be thought of as a pseudo residual or proxy residual.\n",
    "\n",
    "* Now, instead of taking residual at every stage, we can directly the proxy residual value. \n",
    "    - From this, we can apply any model (loss function) for boosting as long as it is differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "* Wiki article → https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "\n",
    "<!-- ![gradient_boosting_algo](https://user-images.githubusercontent.com/63333753/125189670-d98ea080-e256-11eb-9d72-f311df31af48.PNG)\n",
    "\n",
    "* Helpful article → https://explained.ai/gradient-boosting/index.html\n",
    "\n",
    "**Credits** - Image from Wiki -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input: training set $\\{(x_i, y_i)\\}_{i=1}^n$, a differentiable loss function $L\\big(y, F(x)\\big)$, number of iterations $M$.\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Initialize model with a constant value:\n",
    "\n",
    "$$F_0(x) = \\text{argmin}_{\\gamma} \\sum_{i=1}^n L(y_i, \\gamma).$$\n",
    "\n",
    "2. For $m = 1$ to $M$:\n",
    "    - Compute so-called pseudo-residuals:\n",
    "$$r_{im} = - \\bigg[\\frac{\\partial L \\big(y_i, F(x_i)\\big)}{\\partial F(x_i)}\\bigg]_{F(x)=F_{m-1}(x)} \\ \\text{for} \\ i= 1, \\dots, n$$\n",
    "\n",
    "    - Fit a base learner (or weak learner, e.g, tree) $h_m(x)$ to pseudo-residuals, i.e. train it using the training set $\\{(x_i, r_{im})\\}_{i=1}^n$.\n",
    "    \n",
    "    - Compute multiplier $\\gamma_m$ by solving the following one-dimensional optimization (check wiki) problem:\n",
    "$$\\gamma_m = \\text{argmin}_{\\gamma} \\sum_{i=1}^n L\\big(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\big).$$\n",
    "\n",
    "    - Update the model:\n",
    "$$F_m(x) = F_{m-1}(x) + \\gamma_mh_m(x).$$\n",
    "\n",
    "3. Output $F_m(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shrinkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The final model is → $F_M(x) = h_0(x) + \\sum_{m=1}^M \\gamma_m h_m(x)$\n",
    "\n",
    "* $M$ is nothing but the total number of models. This is hyperparameter and the best value should be found by cross-validation.\n",
    "\n",
    "* Shinkage is just an extra parameter which is basically a learning rate ($v$). If we apply shrinkage to the final model at stage `M`, we will have -\n",
    "\n",
    "$$F_M(x) = F_{m-1}(x) + v * \\gamma_m h_m(x), 0 < v \\leq 1$$\n",
    "\n",
    "* $M$ and $v$ are hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting - Training & Runtime Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training**\n",
    "    - $O[n \\log(m) * M]$\n",
    "    - $M \\rightarrow$ total number of base learners\n",
    "    - GBDT is not trivially parallelizable\n",
    "    - Takes more time to train than RandomForest algo.\n",
    "* **Runtime**\n",
    "    - $O(depth * M)$\n",
    "    - Depth is smaller in GBDT than RandomForest algo.\n",
    "* **Space**\n",
    "    - $O(DT + \\gamma_m)$\n",
    "\n",
    "GBDT algos are extensively used at internet companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a popular boosting algo very similar to GBDT.\n",
    "\n",
    "* It is typically used in image processing and computer vision applications.\n",
    "    - Especially in the case of face detection applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking Models\n",
    "\n",
    "Helpful link → http://rasbt.github.io/mlxtend/user_guide/classifier/StackingClassifier/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data (on whole) is trained by base learners at first.\n",
    "\n",
    "* We then generate $X'$ which is actually the output obtained from base learners.\n",
    "\n",
    "* At the last step we take $X'$ and $y$ to return the ensembel model to get final predctions.\n",
    "\n",
    "![ensemble-3](https://user-images.githubusercontent.com/63333753/125252910-83813200-e316-11eb-9c98-314d4fe0b180.png)\n",
    "\n",
    "**Stacking**\n",
    "\n",
    "* Making predictions of a number of models in a hold-out set and then using a different model (meta) to train on those predictions.\n",
    "\n",
    "* Helpful video → https://www.youtube.com/watch?v=enEerl0feRo&ab_channel=HasanShaukat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Stacking Classifier - Scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'col{}'.format(i) : X[:, i] for i in range(len(X[0]))}\n",
    "data['y'] = y\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col0</th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "      <th>col3</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col0  col1  col2  col3  y\n",
       "0   5.1   3.5   1.4   0.2  0\n",
       "1   4.9   3.0   1.4   0.2  0\n",
       "2   4.7   3.2   1.3   0.2  0\n",
       "3   4.6   3.1   1.5   0.2  0\n",
       "4   5.0   3.6   1.4   0.2  0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackingClassifier():\n",
    "    def __init__(self, df, label, base_models, meta_model):\n",
    "        train_df, test_df = self.splitter(dframe=df)\n",
    "        train_df, cv_df = self.splitter(dframe=train_df)\n",
    "        \n",
    "        self.X_train, self.y_train = self.split_features_targets(df=train_df, label=label)\n",
    "        self.X_train = self.X_train.values\n",
    "        self.y_train = self.y_train.values\n",
    "        \n",
    "        self.X_cv, self.y_cv = self.split_features_targets(df=cv_df, label=label)\n",
    "        self.X_cv = self.X_cv.values\n",
    "        self.y_cv = self.y_cv.values\n",
    "        \n",
    "        self.X_test, self.y_test = self.split_features_targets(df=test_df, label=label)\n",
    "        self.X_test = self.X_test.values\n",
    "        self.y_test = self.y_test.values\n",
    "        \n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.cv_preds, self.test_preds = self.fit(base_models=self.base_models)\n",
    "    \n",
    "    def split_features_targets(self, df, label):\n",
    "        X = df.drop(columns=[label], axis=1)\n",
    "        y = df[label]\n",
    "        return X, y\n",
    "    \n",
    "    def splitter(self, dframe, percentage=0.8, random_state=True):\n",
    "        if random_state:\n",
    "            dframe = dframe.sample(frac=1)\n",
    "\n",
    "        thresh = round(len(dframe) * percentage)\n",
    "        train_df = dframe.iloc[:thresh]\n",
    "        test_df = dframe.iloc[thresh:]\n",
    "\n",
    "        return train_df, test_df\n",
    "    \n",
    "    def fit(self, base_models):\n",
    "        cv_preds = {}\n",
    "        test_preds = {}\n",
    "        \n",
    "        for i in range(len(base_models)):\n",
    "            clf = base_models[i]\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            cv_preds['cv_preds{}'.format(i)] = clf.predict(self.X_cv)\n",
    "            test_preds['test_preds{}'.format(i)] = clf.predict(self.X_test)\n",
    "        \n",
    "        return cv_preds, test_preds\n",
    "    \n",
    "    def predict(self):\n",
    "        stacked_cv_preds = np.column_stack(tup=list(self.cv_preds.values()))\n",
    "        stacked_test_preds = np.column_stack(tup=list(self.test_preds.values()))\n",
    "        \n",
    "        meta_model.fit(stacked_cv_preds, self.y_cv)\n",
    "        preds = meta_model.predict(stacked_test_preds)\n",
    "        \n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = KNeighborsClassifier(n_neighbors=3)\n",
    "clf2 = RandomForestClassifier()\n",
    "clf3 = GaussianNB()\n",
    "\n",
    "base_models = [clf1, clf2, clf3]\n",
    "meta_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = StackingClassifier(\n",
    "    df=df, \n",
    "    label='y', \n",
    "    base_models=base_models, \n",
    "    meta_model=meta_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sm.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 1, 2, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 1, 1, 0, 1, 2, 0,\n",
       "       2, 2, 1, 0, 0, 2, 1, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cascading Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* They are typically used when the cost of making a mistake is high.\n",
    "    - Credit card transactions\n",
    "    - Medical domain\n",
    "\n",
    "**Process**\n",
    "\n",
    "* Train the model with whole training data in the first stage.\n",
    "    - Approve those points which are perfectly predicted.\n",
    "    - Remove those points that have been approved from the original training data.\n",
    "\n",
    "* Let the modified training data be again trained on a new model in the second stage.\n",
    "    - Approve those points which are perfectly predicted.\n",
    "    - Remove those points that have been approved from the modified training data.\n",
    "\n",
    "![ensemble-4](https://user-images.githubusercontent.com/63333753/125274793-ecbf7000-e32b-11eb-8da1-c286e04b8111.png)\n",
    "\n",
    "* Continue this process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
