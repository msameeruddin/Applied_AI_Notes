{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Support Vector Machine - (SVM)</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SVM is a popular machine learning technique which is used for both classification and regresstion techniques.\n",
    "\n",
    "![svm-1](https://user-images.githubusercontent.com/63333753/122209692-6cd7ee80-cec2-11eb-9ac7-1e0ddd054bc2.png)\n",
    "\n",
    "* Given positive points and negative points, we can separate them categorically by drawing various hyperplanes. Byt the key idea is to have a plane that can separate the points as widely as possible based on the category (here `P1`, `P2`, `P3` are the planes).\n",
    "\n",
    "* Considering this fact, we can modify the above diagram having the one plane that can separate the points as widely as possible.\n",
    "\n",
    "![svm-2](https://user-images.githubusercontent.com/63333753/122210543-639b5180-cec3-11eb-8a5d-5969df4dfa48.png)\n",
    "\n",
    "* The plane `P3` is thus known as the `margin-maximizing` hyperplane.\n",
    "\n",
    "![svm-3](https://user-images.githubusercontent.com/63333753/122212900-fb01a400-cec5-11eb-92af-dff4f6270fce.png)\n",
    "\n",
    "* From the above graph we get -\n",
    "    - `d` → distance between `P+` and `P-`\n",
    "    - `P+` → the plane touching the first positive point\n",
    "    - `P-` → the plane touching the first negative point\n",
    "\n",
    "> The main agenda of SVM to have distance `d` to be maximum.\n",
    "\n",
    "* The points or vectors from which `P+` and `P-` pass through are called as **support vectors**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Geometric Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given positive points and negative points, draw convex polygons (convex hulls) for both the class points.\n",
    "\n",
    "![svm-4](https://user-images.githubusercontent.com/63333753/122215252-9eec4f00-cec8-11eb-8be9-3c8db83dcc3f.png)\n",
    "\n",
    "* Find the shortest line connecting these two convex polygons (convex hulls).\n",
    "\n",
    "![svm-5](https://user-images.githubusercontent.com/63333753/122215668-1a4e0080-cec9-11eb-93b2-dbff80889c98.png)\n",
    "\n",
    "* Bisect the shortest line into two halves. On the point of bisection, draw a plane which ultimately separates two classes.\n",
    "\n",
    "![svm-6](https://user-images.githubusercontent.com/63333753/122218592-399a5d00-cecc-11eb-9462-860d61b695e4.png)\n",
    "\n",
    "* Thus, you get plane that can separate the classes as widely as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Derivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We all know that `P` is is the margin-maximizing hyperplane and therefore $P : w^TX + b = 0$ and $(w^Tw \\neq 1)$.\n",
    "\n",
    "![svm-7](https://user-images.githubusercontent.com/63333753/122352677-f47a3780-cf6c-11eb-9002-9939328488cc.png)\n",
    "\n",
    "* From the diagram we can have\n",
    "    - $P+ : w^TX + b = +1$\n",
    "    - $P- : w^TX + b = -1$\n",
    "\n",
    "* By simple quadratic geometry, we can write $\\big(d = \\frac{2}{||w||}\\big)$ and `d` is called a margin.\n",
    "\n",
    "* The main goal is to find \n",
    "\n",
    "$$(w^*, b^*) = argmax_{w, b} \\ \\frac{2}{||w||}$$\n",
    "\n",
    "* Here, blue points represent positive class and orange points represent negative class. The $y_i$ for positive class is $+1$ and for negative class it is $-1$.\n",
    "\n",
    "* **Constraints**\n",
    "    - In positive class, the support vector and non-support vector - the value of $\\big[y_i(w^Tx_i + b) \\geq 1\\big] \\rightarrow (+*+ = +)$\n",
    "    - In negative class, the support vector and non-support vector - the value of $\\big[y_i(w^Tx_i + b) \\geq 1\\big] \\rightarrow (-*- = +)$\n",
    "\n",
    "* Ultimately we have to find \n",
    "\n",
    "$$(w^*, b^*) = argmax_{w, b} \\ \\frac{2}{||w||} \\ \\text{such that} \\ \\forall \\ i \\ \\big[y_i(w^Tx_i + b) \\geq 1\\big] \\rightarrow (1)$$\n",
    "\n",
    "* $(1)$ works well when the data is linearly separable. It may not do well for non-linear separable data.\n",
    "\n",
    "![svm-8](https://user-images.githubusercontent.com/63333753/122357851-b7647400-cf71-11eb-9ad0-bcc1b61f9778.png)\n",
    "\n",
    "* In the above diagram, there are few misclassified points. Now, considering these misclassified points, we can modify $(1)$. To do so, we introduce a new variable i.e., $\\zeta_i : \\forall \\ i$ such that, if a positive point lies above `P+` then $(\\zeta_i = 0)$ and if a negative point like below `P-` then $(\\zeta_i = 0)$. Similaryly, for misclassified points, the $\\zeta_i$ will be greater than $0$.\n",
    "    - If \\zeta_i increases, then the point is farther away from the correct hyperplane in the incorrect direction.\n",
    "    - $\\forall \\ i \\ \\zeta_i = 0 \\ \\text{if} \\ y_i(w^Tx_i + b) \\geq 1$\n",
    "    - $\\forall \\ i \\ \\zeta_i > 0$ and it is equal to some units of distance if it is away from the correct hyperplane in the incorrect direction.\n",
    "\n",
    "* Thus $(1)$ becomes\n",
    "\n",
    "$$(w^*, b^*) = argmin_{w, b} \\ \\bigg[\\frac{||w||}{2} + C\\frac{1}{n} \\sum_{i = 1}^n\\zeta_i\\bigg] \\ \\text{such that} \\ \\forall \\ i \\ \\big[y_i(w^Tx_i + b) \\geq 1 - \\zeta_i \\big] \\ \\text{where} \\ (\\zeta_i \\geq 0) \\rightarrow (2)$$\n",
    "\n",
    "* Now, all that we want to minimize is the errors (average distance of misclassified points) or minimize the number of misclassification. Thus the minimization of $\\zeta_i$ is taken care by $(2)$ and $C$ is the hyperparameter.\n",
    "    - As $C$ increases the model has high variance and thus it overfits.\n",
    "    - As $C$ decreases the model has high bias and thus it underfits.\n",
    "\n",
    "* $(1)$ is often termed as **hard-margin** SVM and $(2)$ is termed as **soft-margin** SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hinge Loss\n",
    "\n",
    "refer to → https://www.youtube.com/watch?v=8xbnLHn4jjQ&ab_channel=ArtificialIntelligence-AllinOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The soft-margin equation (primal equation) is\n",
    "\n",
    "$$(w^*, b^*) = argmin_{w, b} \\ \\bigg[\\frac{||w||}{2} + C\\frac{1}{n} \\sum_{i = 1}^n\\zeta_i\\bigg] \\ \\text{such that} \\ \\forall \\ i \\ \\big[y_i(w^Tx_i + b) \\geq 1 - \\zeta_i \\big] \\ \\text{where} \\ (\\zeta_i \\geq 0)$$\n",
    "\n",
    "![hinge_loss](https://user-images.githubusercontent.com/63333753/122391023-3668a500-cf90-11eb-8d51-4fcb418de8f4.png)\n",
    "\n",
    "* The optimization problem can be represented as -\n",
    "\n",
    "$$(w^*, b^*) = argmin_{w, b} \\ \\bigg[\\frac{||w||}{2} + C\\frac{1}{n} \\sum_{i = 1}^n \\text{max}\\big\\{0, 1 - y_i(w^Tx_i + b)\\big\\}\\bigg]$$\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dual Formulation of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The formula is as follows -\n",
    "\n",
    "$$\\text{max}_\\alpha = \\sum_{i=1}^n \\alpha_i - \\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j x_i^Tx_j \\ \\text{such that} \\ \\sum_{i=1}^n \\alpha_i y_i = 0; \\alpha_i \\geq 0$$\n",
    "\n",
    "* Here, $x_i^Tx_j$ is basically a similarity function or kernel function that is defined as $K(x_i, x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The kernel trick is simply the replacement of $x_i^Tx_j$ with the kernel function $K(x_i, x_j)$.\n",
    "\n",
    "* SVM without kernel trick is called linear SVM. Otherwise, it is called as kernel SVM.\n",
    "\n",
    "* In linear SVM, the whole idea is to find the hyperpane that maximizes the margin which is based on $x_i$'s. Whereas, in logistic regression, the idea is to minimize the loss again based on $x_i$'s.\n",
    "\n",
    "![svm-9](https://user-images.githubusercontent.com/63333753/122514116-07ead880-d029-11eb-8791-37896c92cc07.png)\n",
    "\n",
    "* Considering the above diagram, we can have the conclusion as follows.\n",
    "    - Linear SVM fails to classify correctly\n",
    "    - Logistic regression fails to classify correctly\n",
    "    - Logistic regression combined with feature transformation can succeed to classify correctly\n",
    "    - Kernel SVM provided with right kernel can succeed to classify correctly\n",
    "\n",
    "* Kernel SVM with the help of kernel trick can solve classification problem even for non-linearly separable data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Kernalization\n",
    "\n",
    "$$K(x_1, x_2) = (x_1^Tx_2 + C)^d$$\n",
    "\n",
    "* Here, $d$ can be anything and hence it is known as quadratic kernel.\n",
    "\n",
    "$$K(x_1, x_2) = (1 + x_1^Tx_2)^2 \\rightarrow (2)$$\n",
    "\n",
    "* Let $x_1 = [x_{11}, x_{12}]$ and $x_2 = [x_{21}, x_{22}]$. If we expand $(2)$, we get -\n",
    "\n",
    "$$(2) \\implies K(x_1, x_2) = [1 + x_{11}x_{21} + x_{12}x_{22}]^2$$\n",
    "\n",
    "$$\\implies K(x_1, x_2) = [1 + x_{11}^2x_{21}^2 + x_{12}^2x_{22}^2 + 2x_{11}x_{21} + 2x_{12}x_{22} + 2x_{11}x_{21}x_{12}x_{22}] \\rightarrow (3)$$\n",
    "\n",
    "* $(3)$ can be split into two vectors like -\n",
    "\n",
    "$$x_1^1 = [1, x_{11}^2, x_{12}^2, \\sqrt{2} \\ x_{11}, \\sqrt{2} \\ x_{12}, \\sqrt{2} \\ x_{11}x_{12}]$$\n",
    "\n",
    "$$x_2^1 = [1, x_{21}^2, x_{22}^2, \\sqrt{2} \\ x_{21}, \\sqrt{2} \\ x_{22}, \\sqrt{2} \\ x_{21}x_{22}]$$\n",
    "\n",
    "* We can say that ${(x_1^1)}^T(x_2^1) = (3)$.\n",
    "\n",
    "* Internally, kernalization is also doing the feature transformation except that kernalization is based upon similiarity between two points whereas feature transformation is just transforming the feature independently.\n",
    "\n",
    "* Kernalization is taking the data from $d$ space and implicitly transforming it internally to a $d^1$ space where $(d^1 > d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function (Kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most popular SVM kernel is RBF kernel. This is a general purpose kernel which works most of the time.\n",
    "\n",
    "* Given $(x_1, x_2)$, the RBF kernel is written as \n",
    "\n",
    "$$K_{RBF} (x_1, x_2) = \\exp\\bigg[\\frac{-||x_1 - x_2||^2}{2\\sigma^2}\\bigg]$$\n",
    "\n",
    "* Here\n",
    "    - $-||x_1 - x_2||^2$ is distance\n",
    "    - $\\sigma$ is hyperparameter\n",
    "\n",
    "* As distance increases the kernel $K(x_1, x_2)$ decreases.\n",
    "\n",
    "* RBF looks very similar to Gaussian PDF.\n",
    "\n",
    "* When $(\\sigma = 1)$\n",
    "\n",
    "![rbf-s1](https://user-images.githubusercontent.com/63333753/122547097-498d7a80-d04d-11eb-8f4d-5d988bb2ab47.PNG)\n",
    "\n",
    "* When $(\\sigma = 0.1)$\n",
    "\n",
    "![rbf-s0 1](https://user-images.githubusercontent.com/63333753/122547202-6b86fd00-d04d-11eb-81b8-f96bada2d26e.PNG)\n",
    "\n",
    "* When $(\\sigma = 10)$\n",
    "\n",
    "![rdf-s10](https://user-images.githubusercontent.com/63333753/122547322-91140680-d04d-11eb-827c-fb26342d1131.PNG)\n",
    "\n",
    "* Intuitively, we can say that dissimilarity arises when there is so much distance. Similarly, similarity arises when there is less distance.\n",
    "    - Dissimilarity $\\equiv$ Distance\n",
    "    - Similarity $\\equiv$ Kernel\n",
    "\n",
    "* **Relationship b/w KNN & RBF**\n",
    "\n",
    "    - When $\\sigma$ increases, it is roughly equivalent to increase of $k$ in KNN.\n",
    "    - On the contrary, when $\\sigma$ decreases, it is same as decrease of $k$ in KNN.\n",
    "\n",
    "> When we do not know which kernel to use for certain classification problem, then it is recommended to use RBF.\n",
    "\n",
    "* In the case of using RBF-SVM, ultimately we have two hyperparameters - $(C, \\sigma)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Runtime Complexity of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can train the SVM by a regular stochastic gradient descent algorithm. However, there are specialized algorithms that are maninly designed to tackle dual problems - **Sequential Minimal Optimization (SMO)**.\n",
    "\n",
    "* `libsvm` - open source library to train SVM's.\n",
    "\n",
    "* Training time of SVM is roughly $O(n^2)$ for kernel SVM's.\n",
    "\n",
    "* More practitioners do not use SVM when `n` is large.\n",
    "\n",
    "**Runtime complexity**\n",
    "\n",
    "* Given $x_q$ to find $y_q$, we know that $f(x_q) = \\sum_{i=1}^n \\alpha_i y_i K(x_i, x_q) + b$ and $(\\alpha_i = 0)$ for non-support vectors. So, ultimately the runtime depends on number of support vectors.\n",
    "\n",
    "* If number of support vectors is $k$, then time complexity is $O(kd)$ given $(1 \\leq k \\leq n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The SVR formulation is very simple.\n",
    "\n",
    "$$\\text{SVR} \\implies \\frac{||w||^2}{2} \\ \\text{such that} \\ |y_i - \\hat{y_i}| \\leq \\epsilon \\ \\text{and} \\ \\epsilon \\geq 0 \\ \\text{where} \\ \\hat{y_i} = (w^Tx_i + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Feature engineering & Feature transforming** → Kernel designs (rbf or appropriate kernel)\n",
    "* **Decision surface** → Hyperplane for Linear SVM, Linear surface in case of Kernel SVM as the data of `d` dimensions converted into the data of `d'` dimensions.\n",
    "* **Similarity / Distance function** → can easily be converted into kernel with few tweaks. Once the kernel is obtained, SVM is straight forward.\n",
    "* **Interpretability & Feature importance** → No specific way to get the important features except that we need to use forward feature selection.\n",
    "* **Ouliers** → Very less impact for SVM as support vectors are important. But RBF SVM with small $\\sigma$ value can impact the model and may be prone to outliers.\n",
    "* **Bias variance tradeoff** → If C increases the model overfits and underfits if it is less. In the case of RBF, C & $\\sigma$ are important.\n",
    "* **Large dimensional space** → Works like a champ if the right kernel is authorized.\n",
    "\n",
    "* **Best case** → Right kernel\n",
    "* **Worst case** → `n` and number of support vectors is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
