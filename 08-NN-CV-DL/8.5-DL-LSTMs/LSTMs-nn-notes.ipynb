{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why RNNs?\n",
    "\n",
    "Recurrent Neural Network (retains and the leverages the sequence information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We know that the architecture of **CNN** is different from the architecture of **MLP**.\n",
    "    - **MLP** works very well if the input data is a vector\n",
    "    - **CNN** workd very well if the input data is an image\n",
    "\n",
    "* What if the input is a **sequence**?\n",
    "    - sequence of words (amazon food reviews)\n",
    "        - this phone is very smart (sequene is very important)\n",
    "        - when we apply **BoW**, **TFIDF**, **W2V** the context and the sequence is lost\n",
    "    - when we have time series data (sequence prediction problem)\n",
    "        - stock market prediction\n",
    "        - windowing the data based on the interval\n",
    "        - frequency domain (fourier transformation)\n",
    "    - machine translation (language)\n",
    "        - english to spanish (sequence information)\n",
    "    - speech recogniation (**siri**, **alexa**, **google assistant**)\n",
    "    - image captioning\n",
    "        - input is the image\n",
    "        - output is a sentence describing the image\n",
    "        - google image search\n",
    "\n",
    "* RNNs are very important as we use sequence information there by building robust models.\n",
    "    - output completely depends on the sequence of the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Network\n",
    "\n",
    "structure and example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recurrent means repeating\n",
    "\n",
    "* Amazon Food Reviews → binary classification task\n",
    "    - $D = \\{x_i, y_i\\}$\n",
    "    - $x_i = \\text{sequence of words or a sentence that has a context}$\n",
    "    - $y_i = \\text{binary class - positive or negative}$\n",
    "    - words are represented in a One-Hot-Encoding way\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    - $x_1 \\rightarrow \\ <x_{11}, x_{12}, x_{13}, x_{14}, x_{15}>$ and $y_1$\n",
    "    \n",
    "    ![rnn-structure-example](https://user-images.githubusercontent.com/63333753/143395161-0bf024bf-fcf4-4355-ba32-d68575fe0f5d.jpeg)\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    - to get the $y_1$, take $O_5$ and connect it to softmax or logistic layer. We will get the prediction\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    ![simple-rnn-structure](https://user-images.githubusercontent.com/63333753/143396318-ff5598e1-40f4-4f12-9bf6-3ab45d3f92fc.jpeg)\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    - to create that repeatitive struture, we can place $O_0$ which is a set of zeros or random numbers\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    ![simple-rnn-structure-ano](https://user-images.githubusercontent.com/63333753/143396829-557292fa-3a56-4eb6-8522-386751b1002b.jpeg)\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    - the final structure is as follows\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    <img src=\"https://www.researchgate.net/profile/Weijiang-Feng/publication/318332317/figure/fig1/AS:614309562437664@1523474221928/The-standard-RNN-and-unfolded-RNN.png\">\n",
    "    \n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Over Time\n",
    "\n",
    "backpropagation in RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fb_prop_rnn](https://user-images.githubusercontent.com/63333753/143411274-583b7035-5bdb-4b25-93a9-abebabca1924.jpeg)\n",
    "\n",
    "From the above image we can formulate -\n",
    "\n",
    "**Forward Propagation** (weights do not change and follow the arrows)\n",
    "\n",
    "* $O_1 = f(wx_{i1} + w^1O_0)$\n",
    "\n",
    "* $O_2 = f(wx_{i2} + w^1O_1)$\n",
    "\n",
    "* ...\n",
    "\n",
    "* $O_{10} = f(wx_{i10} + w^1O_9)$\n",
    "\n",
    "* $\\hat{y_i} = g(w^{11}O_{10})$\n",
    "\n",
    "**Backward Propagation** (weights do change and follow the opposite direction of arrows)\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial \\hat{y_i}}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial O_{10}} = \\frac{\\partial L}{\\partial \\hat{y_i}}\\frac{\\partial \\hat{y_i}}{\\partial O_{10}}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w^{11}} = \\frac{\\partial L}{\\partial \\hat{y_i}}\\frac{\\partial \\hat{y_i}}{\\partial w^{11}}$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y_i}}\\frac{\\partial \\hat{y_i}}{\\partial O_{10}}\\frac{\\partial O_{10}}{\\partial w} \\ (\\text{for the 1st w from backwards})$$\n",
    "\n",
    "* By this, we will update all $\\{w, w^1, w^{11}\\}$ and this happes for single sequence in time.\n",
    "* The only problem with this is that, if the partial derivative value is less than 1 then it can lead to vanishing gradient problem.\n",
    "    - this is happening not because we have so many layers but with so many sequences\n",
    "    - LSTMs and GRUs are mainly designed to overcome this problem\n",
    "\n",
    "---\n",
    "\n",
    "- Can we take an intuition of RNN with simple cooking analogy like:\n",
    "    - If we are cooking a dish, we take a single pan (function) to which we add different ingredients (inputs) one by one in a sequence but the pan remains same.\n",
    "    - During this everytime a new ingredient is added flavour (output) gets changed.\n",
    "    - Back propogation is like after finishing the dish we try to taste the sample if we feel that dish does not have enough salt (w, w') we add a little salt and check if the dish is perfect now if not we will add it again till the dish tastes perfect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **One to One RNN**\n",
    "    - also called as simple MLP\n",
    "    - one input (no sequence)\n",
    "    - one output (no sequence)\n",
    "\n",
    "* **Many to One RNN**\n",
    "    - regular RNN with sequence of inputs and one output\n",
    "    - very much useful for sentiment classification\n",
    "    \n",
    "<!--     <img src=\"https://www.researchgate.net/profile/Yishan-Jiao/publication/307889236/figure/fig1/AS:614056591388688@1523413908449/Many-to-one-RNN-structure-used-in-the-method-of-DNN-with-RNNon-sequence.png\"> -->\n",
    "    \n",
    "\n",
    "* **One to Many RNN**\n",
    "    - one input (no sequence) - can be an image\n",
    "    - the output can be sequence\n",
    "    - very much useful for image captioning\n",
    "    \n",
    "<!--     <img src=\"https://www.simplilearn.com/ice9/free_resources_article_thumb/One_to_Many_RNN.png\"> -->\n",
    "    \n",
    "\n",
    "* **Many to Many RNN (A)**\n",
    "    - the input and output are of same length\n",
    "    - the input is a sequence\n",
    "    - the output is also a sequence\n",
    "    - very much useful in indentifying the parts of speech given a sentence\n",
    "        - the input can be sentence (sequence)\n",
    "        - the output again is a sequence (set of parts of speech)\n",
    "\n",
    "* **Many to Many RNN (B)**\n",
    "    - the input and output are of different length\n",
    "    - the input is a sequence\n",
    "    - the output is also a sequence\n",
    "    - very much useful in machine translation\n",
    "        - english sentence can have 10 words whereas spanish can have 6 words\n",
    "    - this is also called as encoder-decoder network\n",
    "\n",
    "<img src=\"https://www.dummies.com/wp-content/uploads/deep-learning-recurrent-neural-network-input-output.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs / GRUs\n",
    "\n",
    "need → https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If the fourth output is dependant on 1st input, it is called **long-term dependency**. The backpropagation may not reach till there.\n",
    "    - the simple RNN cannot take care of long-term dependency\n",
    "\n",
    "* We have a huge long-term dependency especially in the problems like machine translation.\n",
    "\n",
    "* To avoid these problems, developers have come up with LSTMs or GRUs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM\n",
    "\n",
    "Long Short Term Memory RNN - http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It can take care of long-term dependency as well as short-term dependency.\n",
    "\n",
    "* LSTM - 1997\n",
    "    - input gate\n",
    "    - output gate\n",
    "    - forget gate\n",
    "\n",
    "    ![lstm-structure](https://user-images.githubusercontent.com/63333753/143560894-207d3382-d4ee-4d1f-8f58-0ff703ab0402.png)\n",
    "\n",
    "* Two ways of representing a RNN\n",
    "    - left side → addition operation\n",
    "    - right side → concatenation operation\n",
    "    \n",
    "    ![two-ways-rnn](https://user-images.githubusercontent.com/63333753/143535208-a61e40e1-52f9-4edf-ba09-d6ce6b35ca18.jpeg)\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Read the above blog carefully.**\n",
    "\n",
    "    <img src=\"https://i.imgur.com/wM8Wk1Q.png\">\n",
    "\n",
    "* YouTube → https://youtu.be/8HyCNIVRbSU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU - Gated Recurrent Unit\n",
    "\n",
    "https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* GRU - 2014\n",
    "    - simplified and optimized version inspired from LSTM\n",
    "    - faster to train\n",
    "    - as powerful as LSTM\n",
    "    - two gets\n",
    "        - reset gate\n",
    "        - update gate\n",
    "\n",
    "![gru-structure](https://user-images.githubusercontent.com/63333753/143561226-dc36119e-7ca1-4bb9-9e74-c9a9ec5e26d1.png)\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Forward propagation happens via **→**\n",
    "* Backward propagation happens via **←**\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/8ngKs.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-directional RNN\n",
    "\n",
    "BRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Sometimes one output may depend on multiple (previous) inputs as well as multiple (next or following) inputs which the model will see later.\n",
    "    - notation wise, we can write like $y_{it}$ is dependent both on $x_{it}$ and $x_{it + k}$\n",
    "* This is called bidirectional (from both previous and following ends) RNN.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\">\n",
    "\n",
    "* Forward propagation happens via **→**\n",
    "* Backward propagation happens via **←**\n",
    "\n",
    "* This model is mostly used in NLP tasks where a sentiment or output is dependent on bidirectional inputs.\n",
    "\n",
    "* First the model is fed with the first sequence of words on which the output of forward direction is calculated. Then that sequence is reversed and fed into another RNN, and the output is generated at each time step. Now these outputs are concatenated and accordingly the loss is calculated and backpropogated to forward LSTM and backward LSTM.\n",
    "\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
