{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of Neural Networks and DL\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* DL popular part of ML in the society, businesses and what not.\n",
    "\n",
    "* Simplest model of NN is **Perceptron**. Developed in 1957 by **Rosenblatt**.\n",
    "    - can be imagined like a logistic regression with few changes in the loss function.\n",
    "\n",
    "* NN - biological inspiration (loosely inspired from biology)\n",
    "\n",
    "    <img src=\"https://www.researchgate.net/profile/Zhenzhu-Meng/publication/339446790/figure/fig2/AS:862019817320450@1582532948784/A-biological-neuron-in-comparison-to-an-artificial-neural-network-a-human-neuron-b.png\">\n",
    "    \n",
    "    - some inputs are more important than the others and that's why we have weights for every input.\n",
    "\n",
    "* To train a NN, **Backpropagation** algorithm is used. It is just a chain rule with differentiation.\n",
    "\n",
    "* **Examples**\n",
    "    - self-driving cars\n",
    "    - voice assistants\n",
    "        - siri\n",
    "        - cortana\n",
    "        - google assistant\n",
    "\n",
    "* **Activation function**\n",
    "\n",
    "    $$O = f \\bigg(\\sum_{i=1}^n w_ix_i\\bigg)$$\n",
    "    \n",
    "    - $O$ → Output\n",
    "    - $f$ → Activation function\n",
    "    - $w_i$ → Weights\n",
    "    - $x_i$ → Inputs\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron (Logistic Regression) - NN Perspective\n",
    "\n",
    "Simplified model of a single neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In Logistic Regression, we have\n",
    "    - $x_i$ from which we have to predict $y_i$\n",
    "    - $D = \\{x_i, y_i\\} \\implies \\hat{y_i} = \\text{sigmoid}(w^TX + b) \\implies \\hat{y_i} = \\text{sigmoid}(\\sum w_ix_i + b)$\n",
    "\n",
    "* To Logistic Regression in the form of NN, we can have\n",
    "    - $x_i$ from which we have to predict $y_i$\n",
    "    - $D = \\{x_i, y_i\\} \\implies O = f \\big(w^TX + b \\big) \\implies O = f \\big(\\sum w_ix_i + b \\big)$\n",
    "\n",
    "* In Perceptron, the entire concept is similar to Logistic Regression with a slight change in the activation function.\n",
    "    - an activation function is denoted as $f$\n",
    "    ```python\n",
    "    def f(x):\n",
    "        return 1 if (np.dot(w.T, X) + b > 0) else 0\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Layered Perceptron (MLP)\n",
    "\n",
    "A graphical way of representing function compositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A multilayer perceptron (MLP) is a class of feedforward artificial neural network (ANN).\n",
    "\n",
    "* The term MLP is used ambiguously, sometimes loosely to mean any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation).\n",
    "\n",
    "* An MLP consists of at least three layers of nodes: an `input layer`, a `hidden layer` and an `output layer`.\n",
    "\n",
    "* Except for the input nodes, each node is a neuron that uses a nonlinear activation function.\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/800/0*eaw1POHESc--l5yR.png\">\n",
    "\n",
    "* **Why should we care about MLP?**\n",
    "    - Biological inspiration - Neuroscience\n",
    "    - Mathematics - by using multi-layered structures (perceptrons), we can arrive at complex mathematical functions to solve the task.\n",
    "\n",
    "> MLP results in very powerful models. Powerful models tend to overfit easily.\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DL - ANN Notations\n",
    "\n",
    "<a href=\"https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/\" target=\"_blank\">NN configuration</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $x_{ij}$ → $x_i$ is a point that belongs to $j^{th}$ feature.\n",
    "\n",
    "* $f_{ij}$ → $f_i$ is a function in layer $i$ at index $j$.\n",
    "\n",
    "* $w_{ij}^k$ → $k$ stands for the `next layer`; $i$ stands for `from`; $j$ stands for `to`.\n",
    "\n",
    "<!-- ![dl-notations](https://user-images.githubusercontent.com/63333753/138650097-55551ae9-8210-4aae-a71d-7d188104df81.png) -->\n",
    "<img src=\"https://www.appliedaicourse.com/images/eif/62366_1617119575.png\">\n",
    "\n",
    "* The above neural network is a fully connected neural network or fully connected multi-layered perceptron.\n",
    "\n",
    "* Weights matrix can be obtained from the weight values at each layer.\n",
    "\n",
    "![weights_matrices](https://user-images.githubusercontent.com/63333753/138651614-36c586b5-f337-4b44-8d8a-c12c5365095a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Single Neuron Model\n",
    "\n",
    "finding the best edge weights using training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perceptron and Logistic Regression are single neuron models for classification.\n",
    "\n",
    "* Linear Regression is a single neuron model for regression analysis.\n",
    "\n",
    "![train-snn](https://user-images.githubusercontent.com/63333753/138656625-def6f7ef-d432-4526-9bbf-98ffb4678d1e.PNG)\n",
    "\n",
    "* Loss function (optimization)\n",
    "\n",
    "    $$w^* = \\text{argmin}_{w} \\sum_{i=1}^n \\big[y_i - f(w^Tx_i)\\big]^2 + \\text{reg}$$\n",
    "    \n",
    "    - solve the optimization problem\n",
    "    - initialization of weights ($w_i$) → randomly\n",
    "    - partial derivatives\n",
    "    - updating the weights (this has to be implemented till it is converged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's say that $D = \\{x_i, y_i\\}; x_i \\in R^4; y_i \\in R \\implies \\text{Standard Regression}$\n",
    "\n",
    "![mlp_training](https://user-images.githubusercontent.com/63333753/138676342-72ba92c2-42b5-44be-ad66-f78cf2ef7f51.png)\n",
    "\n",
    "* We get weights from each layer as -\n",
    "    - $w^1_{4 \\text{x} 3}$ → 12 weights\n",
    "    - $w^2_{3 \\text{x} 2}$ → 6 weights\n",
    "    - $w^3_{2 \\text{x} 1}$ → 2 weights\n",
    "\n",
    "* To train a MLP or compute the weights, we need to follow the steps below:\n",
    "    - define a loss function as\n",
    "    \n",
    "    $$L = \\sum_{i=1}^n(y_i - \\hat{y_i})^2 + \\text{reg}$$ similary for a single point, the loss function seems to be like\n",
    "    $$L_i = (y_i - \\hat{y_i})^2$$\n",
    "    \n",
    "    - optimization problem look like\n",
    "    \n",
    "    $$\\text{min}_{w^k_{ij}} L$$\n",
    "    \n",
    "    - Stochastic Gradient Descent or (any) Gradient Descent\n",
    "    \n",
    "    $$\\frac{\\partial L}{\\partial w^k_{ij}}$$\n",
    "        - initialization of variables $(w^k_{ij})$ → randomly\n",
    "        - updating the weights\n",
    "        $$\\big[w^k_{ij}\\big]_{\\text{new}} = \\big[w^k_{ij}\\big]_{\\text{old}} - \\alpha \\bigg[\\frac{\\partial L}{\\partial w^k_{ij}}\\bigg]$$\n",
    "        - continue the process of updating till convergence\n",
    "\n",
    "<br>\n",
    "\n",
    "* For $w^3_{2 \\text{x} 1}$\n",
    "    \n",
    "    * $w^3_{11}$ using chain rule\n",
    "\n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138678028-2bf231d3-737d-4b07-830d-2e532b030083.png)\n",
    "\n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^3_{11}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial w^3_{11}}$$\n",
    "    \n",
    "    * $w^3_{21}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138678802-4a37bb86-d99d-4536-b1aa-06cb2b53df96.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^3_{21}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial w^3_{21}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* For $w^2_{3 \\text{x} 2}$\n",
    "    \n",
    "    * $w^2_{11}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138679941-7d3895a3-f4f3-43b9-ae6b-86c65e935da0.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^2_{11}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial O_{21}} \\frac{\\partial O_{21}}{\\partial w^2_{11}}$$\n",
    "    \n",
    "    * $w^2_{21}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138680981-38591d97-d091-4255-abe5-3a0dea0b585c.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^2_{21}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial O_{21}} \\frac{\\partial O_{21}}{\\partial w^2_{21}}$$\n",
    "    \n",
    "    * $w^2_{31}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138681459-8e86293c-8ed2-4cfc-a78c-b77222cc996b.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^2_{31}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial O_{21}} \\frac{\\partial O_{21}}{\\partial w^2_{31}}$$\n",
    "    \n",
    "    * $w^2_{12}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138683652-e42651eb-ce9e-4286-a4e6-283099ccfaa0.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^2_{12}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial O_{22}} \\frac{\\partial O_{22}}{\\partial w^2_{12}}$$\n",
    "    \n",
    "    * $w^2_{22}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138683897-c498440d-292d-43df-9467-af2e2d000bcb.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^2_{22}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial O_{22}} \\frac{\\partial O_{22}}{\\partial w^2_{22}}$$\n",
    "    \n",
    "    * $w^2_{32}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138684182-8469314f-c54b-4951-ba83-469deb02203e.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^2_{32}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{31}}{\\partial O_{22}} \\frac{\\partial O_{22}}{\\partial w^2_{32}}$$\n",
    "\n",
    "* For $w^1_{4 \\text{x} 3}$\n",
    "    \n",
    "    * $w^1_{11}$ using chain rule\n",
    "    \n",
    "    ![mlp_training](https://user-images.githubusercontent.com/63333753/138687244-08ed5aa4-9564-42c1-a32b-77350beefd7d.png)\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^1_{11}} = \\frac{\\partial L}{\\partial O_{31}} \\bigg\\{\\frac{\\partial O_{31}}{\\partial O_{21}} \\frac{\\partial O_{21}}{\\partial O_{11}} \\frac{\\partial O_{11}}{\\partial w^1_{11}} + \\frac{\\partial O_{31}}{\\partial O_{22}} \\frac{\\partial O_{22}}{\\partial O_{11}} \\frac{\\partial O_{11}}{\\partial w^1_{11}}\\bigg\\}$$\n",
    "    \n",
    "    $$\\text{or}$$\n",
    "    \n",
    "    $$\\implies \\frac{\\partial L}{\\partial w^1_{11}} = \\frac{\\partial L}{\\partial O_{31}} \\frac{\\partial O_{11}}{\\partial w^1_{11}} \\bigg\\{\\frac{\\partial O_{31}}{\\partial O_{21}} \\frac{\\partial O_{21}}{\\partial O_{11}} + \\frac{\\partial O_{31}}{\\partial O_{22}} \\frac{\\partial O_{22}}{\\partial O_{11}}\\bigg\\}$$\n",
    "    \n",
    "    * we have summation because there are two paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memoization\n",
    "\n",
    "compute once and reuse it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Memoization is a method used to store the results of previous function calls to speed up future calculations. If repeated function calls are made with the same parameters, we can store the previous values instead of repeating unnecessary calculations. This results in a significant speed up in calculations.\n",
    "\n",
    "* From the above equations, there are some derivatives which are repeating. Instead of recomputing them, the results can be stored and used to fasten te computation.\n",
    "\n",
    "* It takes slightly more memory but produce the results fastly.\n",
    "\n",
    "![memoization](https://user-images.githubusercontent.com/63333753/138693993-13d3b066-7308-41a7-b10c-c4b8da090bfb.jpeg)\n",
    "\n",
    "**Credits** - Image from AAIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "chain rule + memoization → https://bit.ly/2XFD4xQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have data in the form of $D = \\{x_i, y_i\\}$, we have to send each $x_i$ as input to the neural network.\n",
    "\n",
    "Backpropagation only works iff the activation function are differentiable.\n",
    "\n",
    "* Initialize the parameters (weights) $w^k_{ij}$.\n",
    "\n",
    "* ```python\n",
    "for each x_i in D:\n",
    "    pass x_i forward through the network # forward propagation\n",
    "    # at the end, we will get loss\n",
    "    compute the loss L(y_i, y_i^)\n",
    "    compute all the derivatives using chain rule and memoization\n",
    "    update weights from end of the network to the start # backward propagation\n",
    "```\n",
    "\n",
    "* Repeat the above step till it converges.\n",
    "\n",
    "> In forward propagation, we are sending the inputs and try to compute the output i.e., y_i^. <br>\n",
    "> In backward propagation, we are using the error (loss) to update the weights so that the weights are tuned to reduce the loss.\n",
    "\n",
    "**Epoch** - passing all the data points once through neural network.\n",
    "\n",
    "**Mini-Batch Backpropagation** - is the most popular approach to train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions\n",
    "\n",
    "should be differentiable and easy to differentiate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most popular activation function that were heavily used during 1980 and 1990 are -\n",
    "    - `sigmoid` $(\\sigma)$ - The `sigmoid` function is one of many possible functions that are used as a nonlinear activation function between layers of a neural network.\n",
    "        * $z = w^Tx$\n",
    "        * $\\sigma{(z)} = \\frac{1}{(1 + e^{-z})}$\n",
    "        * $\\frac{d \\sigma{(z)}}{dz} = \\sigma{(z)}[1 - \\sigma{(z)}]$\n",
    "    \n",
    "        <img src=\"http://ronny.rest/media/blog/2017/2017_08_10_sigmoid/sigmoid_and_derivative_plot.jpg\">\n",
    "    \n",
    "        $$0 \\leq \\frac{d \\sigma{(z)}}{dz} < 1$$\n",
    "    \n",
    "        * reference → http://ronny.rest/blog/post_2017_08_10_sigmoid/\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    - `tanh` - The `tanh` function is just another possible functions that can be used as a nonlinear activation function between layers of a neural network. It actually shares a few things in common with the `sigmoid` activation function.\n",
    "        * $z = w^Tx$\n",
    "        * $\\tanh{(z)} = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "        * $\\frac{d \\tanh{(z)}}{dz} = 1 - \\tanh^2{(z)}$\n",
    "        \n",
    "        <img src=\"http://ronny.rest/media/blog/2017/2017_08_16_tanh/tanh_and_gradient.jpg\">\n",
    "        \n",
    "        $$0 \\leq \\frac{d \\tanh{(z)}}{dz} \\leq 1$$\n",
    "        \n",
    "        * reference → http://ronny.rest/blog/post_2017_08_16_tanh/\n",
    "    \n",
    "    <br>\n",
    "    \n",
    "    - `ReLu` - most popular activation function that is extensively used for training deep learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <img src=\"https://qph.fs.quoracdn.net/main-qimg-65a7c3bf75549bad04875d0e789bb5bf\">\n",
    "\n",
    "**Credits** - Image from Internet -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanishing Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **vanishing gradient** problem is encountered when training artificial neural networks with gradient-based learning methods and backpropagation.\n",
    "\n",
    "    * Each of the neural network's weights receives an **update** proportional to the **partial derivative of the error function** with respect to the **current weight** in each iteration of training.\n",
    "\n",
    "    * The problem is that in some cases, the gradient will be **vanishingly small**, effectively preventing the weight from changing its value. In the worst case, this **may completely stop** the neural network from further **training**.\n",
    "\n",
    "* This is often seen when the activation function is either `sigmoid` or `tanh` and **chain rule** multiplication during backpropagation.\n",
    "\n",
    "$$\\text{Vanishing Gradient} \\implies \\frac{\\partial L}{\\partial w^k_{ij}} \\rightarrow \\text{v.v. small}$$\n",
    "\n",
    "    thus, making\n",
    "\n",
    "$$\\big(w^k_{ij}\\big)_{\\text{new}} \\simeq \\big(w^k_{ij}\\big)_{\\text{old}}$$\n",
    "\n",
    "    and we know that\n",
    "    \n",
    "$$\\big(w^k_{ij}\\big)_{\\text{new}} = \\big(w^k_{ij}\\big)_{\\text{old}} - \\alpha \\bigg[\\frac{\\partial L}{\\partial w^k_{ij}}\\bigg]$$\n",
    "\n",
    "* **ReLu** activation function was discovered to avoid the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The **exploding gradient** problem is encountered when training artificial neural network with gradient-based learning methods and backpropagation.\n",
    "    * Each of the neural network's weights receives an **update** proportional to the **partial derivative of the error function** with respect to the **current weight** in each iteration of training.\n",
    "    * The problem is that in some cases, the gradient will be **explodingly large**, effectively producing huge difference. In the worst case, this has the effect of your model being **unstable and unable to learn** from training data.\n",
    "\n",
    "* This is often seen when the activation function either `sigmoid` or `tanh` and **chain rule** multiplication during the backpropagation.\n",
    "\n",
    "$$\\text{Exploding Gradient} \\implies \\frac{\\partial L}{\\partial w^k_{ij}} \\rightarrow v.v. large$$\n",
    "\n",
    "    thus, making\n",
    "\n",
    "$$\\big(w^k_{ij}\\big)_{\\text{new}} >> \\big(w^k_{ij}\\big)_{\\text{old}}$$\n",
    "\n",
    "    and we know that\n",
    "    \n",
    "$$\\big(w^k_{ij}\\big)_{\\text{new}} = \\big(w^k_{ij}\\big)_{\\text{old}} - \\alpha \\bigg[\\frac{\\partial L}{\\partial w^k_{ij}}\\bigg]$$\n",
    "\n",
    "* **ReLu** activation function was discovered to avoid the exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As the number of layers increase, there is a higher chance of overfitting (more weights/params) which can lead to high variance.\n",
    "    - regularization (on weights) can avoid the problem of overfitting (L2)\n",
    "    \n",
    "    $$L = \\sum_{i=1}^n \\text{loss}(y_i, \\hat{y_i}) + \\lambda \\sum_{i, j, k}^n {\\big[w^k_{ij}\\big]}^2$$\n",
    "    \n",
    "    - $\\lambda$ is a hyperparameter\n",
    "    \n",
    "    - number of layers is again a hyperparameter\n",
    "\n",
    "* As the number layers are too low, there is a higher chance of underfitting (less weights/params) which can lead to high bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a href=\"https://playground.tensorflow.org\">playground.tensorflow.org</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
