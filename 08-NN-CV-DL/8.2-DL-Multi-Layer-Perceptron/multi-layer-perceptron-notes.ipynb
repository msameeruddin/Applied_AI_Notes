{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### History of Deep MLP (Problems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In the early 1980's, 1990's, and 2006 people were trying to built 2-3 layered networks. One of the biggest problem they faced was vanishing gradient problem.\n",
    "\n",
    "* If we have more weights or gradients and little data, the deep neural network might easily overfit.\n",
    "\n",
    "* Also, the systems were not computationally powerful to deal with large amounts of data.\n",
    "\n",
    "* In and after 2010, we had lots of data (labelled data). A new processing unit called GPU (Graphical Processing Unit) came into existence which made the processing easy. New ideas and new algorithms and lot of research was put to develop the deep learning methodology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropouts & Regularization (RFR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Deep NN (many layers; many weights to train) → Overfitting\n",
    "\n",
    "* Regularization is one way to control overfitting. Another interesting method is **dropout** (extremely simple and elegant).\n",
    "\n",
    "* In randomforest model, the regularization happens via randomization (because randomforest models tend to overfit easily).\n",
    "\n",
    "* Similarly for deep nn, the regularization via dropout is solely based on randomly selecting the subset of neurons and removing the connections (making them inactive). This, completely resolves the problem of overfitting.\n",
    "\n",
    "* This randomly setting the neurons inactive (dropout) is done for every iteration.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2000/1*S-Rr9boTfKusUzETeKW6Mg.png\">\n",
    "\n",
    "**Credits** - Image from Internet\n",
    "\n",
    "**Dropout Rate**\n",
    "\n",
    "* It is the probablity (p) of the neurons to be inactive in every layer. This value lies between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU - Rectified Linear Units\n",
    "\n",
    "http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Best activation function to solve the problem of vanishing gradients.\n",
    "    - often times, it is used as the default activation function\n",
    "    - replaces sigmoid and tanh activation functions\n",
    "\n",
    "* Let $z = w^Tx$, $\\text{ReLU} → f(z) = z^+ \\implies max(0, z)$ is defined as \n",
    "    - $0 \\ \\text{if} \\ z \\leq 0$\n",
    "    - $z \\ \\text{otherwise}$\n",
    "    - the derivative is either 0 or 1 and thus prevents gradients to be vanished. BUT, there can be **dead activations** as we have 0 to be the lower bound\n",
    "    - **dead activation**\n",
    "        - if $z$ is negative then $f(z) = 0$ and $\\frac{df}{dz} = 0 \\implies$ that the weights will not update. This state is called dead activation.\n",
    "        - to avoid this, we shall use **Leaky ReLU**.\n",
    "    \n",
    "* ReLU converges faster than any other activation functions.\n",
    "\n",
    "* Computational-wise, ReLU is faster than any other activation functions.\n",
    "\n",
    "* **Variants of ReLU**\n",
    "    - https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Rahul-Jayawardana/publication/350567223/figure/fig3/AS:1007855343767554@1617302847631/Fig-3-The-basic-activation-functions-of-the-neural-networksNeural-Networks.jpg\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Initialization of weights $w^k_{ij} = 0 \\forall i, j, k$ → is not a good idea.\n",
    "    - if so, all neurons compute the same thing and there won't be any learning\n",
    "    - same gradients updation happens to neurons\n",
    "    - this ends up with the issue of symmetry when all the weights are same\n",
    "\n",
    "* **Idea 1**\n",
    "    - weights should be small (not too small)\n",
    "    - not all zero values should be there\n",
    "    - there should be good variance → $\\text{Var}(w^k_{ij})$\n",
    "    - $w^k_{ik} \\sim N(0, \\sigma)$ → random initialization with gaussian normal with $\\sigma$ being less\n",
    "\n",
    "* **Are there any better strategies to initialize weights?**\n",
    "    - $\\text{fan}_{\\text{in}}$ → number of inputs  that a neuron has\n",
    "    - $\\text{fan}_{\\text{out}}$ → number of outputs  that a neuron has\n",
    "    \n",
    "    ![faninout](https://user-images.githubusercontent.com/63333753/139575708-1e403a75-0eef-4d1c-aeef-0075b57ee9ca.jpg)\n",
    "    \n",
    "    - initialization of weights should be based on $\\text{fan}_{\\text{in}}$ and $\\text{fan}_{\\text{out}}$ which is commonsensical\n",
    "\n",
    "* **Idea 2**\n",
    "    - uniform initialization of weights using $\\text{fan}_{\\text{in}}$ and $\\text{fan}_{\\text{out}}$\n",
    "    - $w^k_{ij} \\sim \\text{U}\\bigg[\\frac{-1}{\\sqrt{\\text{fan}_{\\text{in}}}}, \\frac{1}{\\sqrt{\\text{fan}_{\\text{in}}}}\\bigg]$\n",
    "    - this technique fairly works for sigmoidal activation functions\n",
    "\n",
    "* **Idea 3**\n",
    "    1. Xavier normal or Glorot normal\n",
    "        - $w^k_{ij} \\sim N(0, \\sigma_{ij})$ where $\\sigma_{ij} = \\sqrt{\\frac{2}{\\text{fan}_{\\text{in}} + \\text{fan}_{\\text{out}}}}$ (this is done for each neuron)\n",
    "    2. Xavier uniform or Glorot uniform\n",
    "        - $w^k_{ij} \\sim U\\bigg[\\frac{-\\sqrt{6}}{\\sqrt{\\text{fan}_{\\text{in}} + \\text{fan}_{\\text{out}}}}, \\frac{\\sqrt{6}}{\\sqrt{\\text{fan}_{\\text{in}} + \\text{fan}_{\\text{out}}}}\\bigg]$\n",
    "    - this technique fairly works for sigmoidal activation functions\n",
    "\n",
    "* **Idea 4**\n",
    "    1. He normal\n",
    "        - $w^k_{ij} \\sim N(0, \\sigma_{ij})$ where $\\sigma_{ij} = \\sqrt{\\frac{2}{\\text{fan}_{\\text{in}}}}$ (this is done for each neuron)\n",
    "    \n",
    "    2. He uniform\n",
    "        - $w^k_{ij} \\sim U\\bigg[\\sqrt{\\frac{-6}{\\text{fan}_{\\text{in}}}}, \\sqrt{\\frac{6}{\\text{fan}_{\\text{in}}}}\\bigg]$\n",
    "    - this technique fairly works for ReLU and Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "https://arxiv.org/pdf/1502.03167v3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The data that is given is $D = \\{x_i, y_i\\}$. As a preprocessing step, we have to do data normalization.\n",
    "\n",
    "**Problem**\n",
    "\n",
    "* When we have fully connected MLP with lots of hidden layers, if we are following mini-batch SGD technique to train the model, then\n",
    "    - if a input changes slightly, it can impact to severe change in the later layers especially if we have a deep network (because of operations that take place)\n",
    "    - this can also be called as internal-covariance-shift\n",
    "        - internal - within the network we see the problem\n",
    "        - covariance - generalization of variance to a vector\n",
    "        - shift - the variance (dispersion) is shifting or changing\n",
    "\n",
    "* <a href=\"https://gradientscience.org/images/batchnorm/dropin.jpg\" target=\"_blank\">Batch normalization</a> helps in faster convergence.\n",
    "    - acts as a (weak) regularizer but dropout is recomended\n",
    "\n",
    "**Solution** (avoids internal covariance shifts)\n",
    "\n",
    "<img src=\"https://gradientscience.org/images/batchnorm/bn_schematic.jpg\">\n",
    "\n",
    "**Algorithm**\n",
    "\n",
    "<!-- <img src=\"https://miro.medium.com/max/1153/1*xQhPvRh08oKFC63swgWr_w.png\"> -->\n",
    "<img src=\"https://miro.medium.com/max/405/1*AdWaQr18d5h5soPS8T7t9w.png\">\n",
    "\n",
    "**Credits** - Images from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill Descent\n",
    "\n",
    "https://medium.com/@ashwin8april/optimization-algorithms-in-deep-learning-4f2c3b53f9f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A SGD technique in optimization is wholly deal with minimizing the loss function. It could so happen that the derivative may become 0 at three criterions.\n",
    "    - a simple SGD or mini-batch SGD could get stuck at saddle point. To avoid that, we use some advanced techniques.\n",
    "\n",
    "    * **convex functions** - have one minima and one maxima. Localminima = global minima\n",
    "    * **non-convex functions** - have multiple local minimas and maximas.\n",
    "\n",
    "* Deep learning deals with non-convex functions where a weight can get stuck (which again depends on the weight initialization) in one local minima unable to reach global minima. Hence, to avoid that we use **Hill climbing descent** technique.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*rdU1ljjrx-QyF9Oi9qnV_Q.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD (Recap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For every iteration the weights need to be updated. The formulation is very simple.\n",
    "\n",
    "$$\\big( w_{ij}^k \\big)_\\text{new} = \\big( w_{ij}^k \\big)_\\text{old} - \\alpha \\bigg[ \\frac{\\partial L}{\\partial ( w_{ij}^k)_\\text{old}} \\bigg] \\rightarrow (1)$$\n",
    "\n",
    "* If we denote $w \\rightarrow w_{ij}^k$, we get\n",
    "\n",
    "$$(1) \\implies w_t = w_{t-1} - \\alpha \\bigg[ \\frac{\\partial L}{\\partial w} \\bigg]_{w_{t-1}} \\rightarrow (2)$$\n",
    "\n",
    "* If we compute $\\big[ \\frac{\\partial L}{\\partial w} \\big]$ using $D = \\{X, y\\}$ all of the data points, we call it as **gradient descent**.\n",
    "\n",
    "* If we compute $\\big[ \\frac{\\partial L}{\\partial w} \\big]$ using $D = \\{x_i, y_i\\}$ one data point (selected at random), we call it as **stochastic gradient descent**.\n",
    "\n",
    "* If we compute $\\big[ \\frac{\\partial L}{\\partial w} \\big]$ using $D = \\{x_i, y_i\\}$ random subset of  data points ($k$ points in $D$), we call it as **mini-batch stochastic gradient descent**.\n",
    "\n",
    "$$\\bigg[ \\frac{\\partial L}{\\partial w} \\bigg]_\\text{mini-batch SGD} \\sim \\bigg[ \\frac{\\partial L}{\\partial w} \\bigg]_\\text{GD} \\implies \\text{not exactly equal but roughly equal}$$\n",
    "\n",
    "* We want to compute gradient descent, but computing SGD or mini-batch SGC is much faster.\n",
    "\n",
    "* The major problem in SGD is that, each of the updates for every iteration, the weights (new) tend to be more noisy.\n",
    "\n",
    "* **How can we de-noise the gradients from SGD so as to converge faster?**\n",
    "    - Batch SGD with momentum!\n",
    "    \n",
    "    <img src=\"https://hackster.imgix.net/uploads/attachments/1109729/_9PfPHqIMBz.blob?auto=compress%2Cformat\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch SGD with Momentum\n",
    "\n",
    "https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d\n",
    "\n",
    "https://ruder.io/optimizing-gradient-descent/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The simple way of denoising the data when given time periods is to take weighted averages or weighted sums.\n",
    "\n",
    "* The most recent point that is seen will be given more weightage than the previous points that had already seen.\n",
    "    - $0 \\leq \\gamma \\leq 1$\n",
    "    - $t = 1; v_1 = a_1$\n",
    "    - $t = 2; v_2 = \\gamma v_1 + a_2$\n",
    "    - $t = 3; v_3 = \\gamma v_2 + a_3 \\implies \\gamma[\\gamma v_1 + a_2] + a_3 \\implies \\gamma^2 a_1 + \\gamma a_2 + a_3$\n",
    "        - if $\\gamma = 0.5 \\implies 0.25 a_1 + 0.5 a_2 + 1 a_3$ \n",
    "        - for the recent value i.e., $a_3$, $\\gamma$ is the highest i.e., 1\n",
    "    - $t = 4; v_4 = 0.125 a_1 + 0.25 a_2 + 0.5 a_3 + 1 a_4$\n",
    "    - ...\n",
    "    - where\n",
    "        - $t_i = \\text{time}$\n",
    "        - $v_i = \\text{values}$\n",
    "        - $a_i = \\text{random numericals}$\n",
    "    - the function can be written as\n",
    "    \n",
    "    $$v_1 = a_1; v_t = \\gamma v_{t - 1} + a_t \\implies \\text{recursive equations}$$\n",
    "    \n",
    "    $$v_t \\sim \\text{denoised estimate at time t}$$\n",
    "    \n",
    "    $$v_t = \\gamma^0 a_t + \\gamma a_{t-1} + \\gamma^2 a_{t-2} + \\gamma^3 a_{t-3} + ... \\gamma^n a_{t-n}$$\n",
    "    \n",
    "    $$1 \\geq \\gamma \\geq \\gamma^2 \\geq \\gamma^3 \\geq ... \\geq \\gamma^n$$\n",
    "    \n",
    "    - this is called **exponential weighting**\n",
    "\n",
    "* w.k.t\n",
    "\n",
    "$$w_t = w_{t - 1} - \\alpha \\bigg[ \\frac{\\partial L}{\\partial w} \\bigg]_{w_{t-1}} \\rightarrow (1)$$\n",
    "\n",
    "* Let's denote $\\big[ \\frac{\\partial L}{\\partial w} \\big]_{w_{t-1}}$ as $g_t$ (gradient at time $t$).\n",
    "\n",
    "* We can write $(1)$ as \n",
    "\n",
    "$$(1) \\implies w_t = w_{t - 1} - \\alpha g_t \\rightarrow (2)$$\n",
    "\n",
    "* Now, we can represent $(2)$ in terms of exponential weighting as \n",
    "\n",
    "    * $v_1 = \\alpha g_t$\n",
    "    * $v_t = \\gamma v_{t-1} + \\alpha g_t$\n",
    "    * $w_t = w_{t-1} - v_t$\n",
    "    * $0 \\leq \\gamma \\leq 1;$ it is recommended to use 0.9 as $\\gamma$\n",
    "    * **Case 1**\n",
    "        - $\\gamma = 0; v_t = \\alpha g_t \\implies w_t = w_{t-1} - \\alpha g_t$\n",
    "    * **Case 2**\n",
    "        - $\\gamma = 0.9; v_t = 0.9 v_{t-1} + \\alpha g_t \\implies w_t = w_{t-1} - \\big[ 0.9 v_{t-1} + \\alpha g_t \\big]$\n",
    "    * **...**\n",
    "\n",
    "* When we use **exponential weighting** to **denoise** the SGD gradients, what we get is **SGD + Momentum**.\n",
    "\n",
    "* What we finally get is \n",
    "$$w_t = w_{t-1} - \\big[ \\gamma v_{t-1} + \\alpha g_t \\big] \\rightarrow (3)$$\n",
    "    \n",
    "    - where\n",
    "        - $\\gamma v_{t-1}$ → momentum\n",
    "        - $\\alpha g_t$ → gradient\n",
    "\n",
    "> SGD + Momentum → speeds up the convergence\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/582/1*fhHakQ1nWN7HK1KBNdarqw.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient (NAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When moving towards a minimum `w` we might overstep from the actual minimum `w` because of the exponentially weighted sum + gradient at the previous point.\n",
    "\n",
    "* [**Image explanation**](https://user-images.githubusercontent.com/63333753/139644226-ba9453b3-7bf7-4566-b2e9-27e6c9bc8d21.jpeg) on why NAG works.\n",
    "\n",
    "* In SGD momentum, we decide the `step` by computing gradient and momentum which can end up in slower convergence.\n",
    "\n",
    "* In NAG, we decide the `step` by computing momentum first and then gradient on that momentum which ends up in faster convergence.\n",
    "\n",
    "<img src=\"https://golden-storage-production.s3.amazonaws.com/topic_images/7a00dcd221e745708101d89f4c4c2a5c.png\">\n",
    "\n",
    "* The equation for NAG is\n",
    "\n",
    "    $$\\text{NAG} \\implies w_t = w_{t-1} - \\big[ \\gamma v_{t-1} + \\alpha g^1 \\big]$$\n",
    "\n",
    "    * where\n",
    "        - $g^1 = \\big[ \\frac{\\partial L}{\\partial w} \\big]_{w^1}$\n",
    "        - $w^1 = w_{t-1} - \\gamma v_{t-1}$\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In SGD, SGD + Momentum, the learning rate ($\\alpha$) is same for each weight.\n",
    "\n",
    "* Can we come up with adaptive learning rate ($\\alpha$) for each weight?\n",
    "    - when data is sparse, keep the learning rate same is not good not optimization\n",
    "        - in sparse data, when a feature is passed as input to a neuron, during the training phase, it can end up having very small value than the actual feature. To avoid that, we go to use adaptive learning rate\n",
    "        - sparser features will result in smaller derivatives\n",
    "\n",
    "* **SGD** representation\n",
    "\n",
    "    $$w_t = w_{t-1} - \\alpha g_t$$\n",
    "    \n",
    "    - the learning rate is same for all the weights\n",
    "\n",
    "* **AdaGrad** representation\n",
    "\n",
    "    $$w_t = w_{t-1} - \\alpha^1_t g_t$$\n",
    "    \n",
    "    - the learning rates are different for each weight\n",
    "    \n",
    "    $$\\alpha^1_t = \\frac{\\alpha}{\\sqrt{c_t + \\epsilon}}$$\n",
    "    \n",
    "    - here \n",
    "        - $\\epsilon$ → small positive number (to avoid division by 0)\n",
    "        - $\\alpha$ → 0.01\n",
    "        - $c_t = \\sum_{i=1}^t g_i^2 \\implies g_i = \\big[ \\frac{\\partial L}{\\partial w} \\big]_{w_{i-1}}$ (because we are taking summ of squares, it can become large value)\n",
    "            - $c_t$ is always positive and $c_t \\geq c_{t-1}$\n",
    "    - as the iteration number increases, the adaptive learning rate decreases (because of the fraction)\n",
    "\n",
    "**Merits**\n",
    "\n",
    "* No need of manually tuning the learning rate, because weights adapt for each iteration.\n",
    "\n",
    "* Works brilliantly for sparse and dense features.\n",
    "\n",
    "**De-Merits**\n",
    "\n",
    "* $c_t$ can become very large as $t$ increases and that may lead to slower convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaDelta & RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* There is a problem of slow convergence in AdaGrad optimizer. This usually happens when $c_t$ is large.\n",
    "\n",
    "* To avoid that problem we use AdaDelta optimizer. The formulation can be seen below -\n",
    "\n",
    "    $$w_t = w_{t-1} - \\alpha_t^1 g_t \\rightarrow (1)$$\n",
    "    \n",
    "    - where\n",
    "        \n",
    "        $$\\alpha_t^1 = \\frac{\\alpha}{\\sqrt{\\text{eda}_{t} + \\epsilon}} \\rightarrow (2)$$\n",
    "        \n",
    "        - $\\text{eda}_t$ is the exponential decaying average at $t$\n",
    "        \n",
    "            $$\\text{eda}_t = \\gamma \\ \\text{eda}_{t-1} + (1 - \\gamma)g^2_{t} \\implies \\text{recursive equation}$$\n",
    "            \n",
    "            - $\\gamma$ is typically 0.95\n",
    "        - it is slightly different form exponential averaging\n",
    "        - $\\text{eda}_t$ is a way to control the growh the denominator term in equation $(2)$ thus avoids the slow convergence\n",
    "\n",
    "* AdaDelta is slightly more organized than AdaGrad.\n",
    "\n",
    "* AdaDelta and RMSProp behave in a similar way though they are different.\n",
    "    - **\" similar though being different \"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam\n",
    "\n",
    "https://arxiv.org/pdf/1412.6980.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Adam → Adaptive Moment Estimation\n",
    "\n",
    "* One of the fastest optimization technique.\n",
    "\n",
    "* The core idea here is to apply $\\text{eda}_t$ for $g_t$.\n",
    "\n",
    "* $\\text{eda} \\ g_t$ can be written as -\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t; 0 \\leq \\beta_1 \\leq 1 \\rightarrow (1)$$\n",
    "\n",
    "* $\\text{eda} \\ g_t^2$ can be written as -\n",
    "\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2; 0 \\leq \\beta_2 \\leq 1 \\rightarrow(1)$$\n",
    "\n",
    "* $(1)$ and $(2)$ are recursive equations. $\\beta_1$ and $\\beta_2$ are typically taken 0.9 and 0.99 respectively.\n",
    "\n",
    "* Similarly, we have other equations such as -\n",
    "\n",
    "$$\\hat{m}_t = \\frac{m_t}{(1 - \\beta_1^t)} \\rightarrow (3); \\ \\hat{v}_t = \\frac{v_t}{(1 - \\beta_2^t)} \\rightarrow (4)$$\n",
    "\n",
    "* From $(3)$ and $(4)$, we get\n",
    "\n",
    "    $$w_t = w_{t-1} - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\rightarrow (5)$$\n",
    "\n",
    "    - in $(5)$, $\\alpha$ is taken as 0.001\n",
    "\n",
    "* Adam has all the advantages of AdaGrad and AdaDelta and works fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "https://cs231n.github.io/neural-networks-3/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If we are working with a small data, then mini-batch SGD would work well.\n",
    "    - Mini-batch-SGD, cannot come out of a saddle point\n",
    "\n",
    "* SGD + Momentum works fairly well but they are slow.\n",
    "\n",
    "* When we have sparse data, AdaGrad is very good.\n",
    "\n",
    "* AdaDelta and Adam outsmarts all the other algorithms.\n",
    "\n",
    "<img src=\"https://ruder.io/content/images/2016/09/saddle_point_evaluation_optimizers.gif\">\n",
    "\n",
    "**Credits** - GIF from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Monitoring & Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The most important part of any optimization strategy is about gradient. It is best to monitor them because, at the end the updation (weights) happens only with gradients.\n",
    "\n",
    "* Monitoring should be done for each epoch, each weight, and each layer so as to acheive the task easily.\n",
    "\n",
    "* Monitoring helps detect problems like vanishing gradients and exploding gradients problems.\n",
    "    - the solution for this is **gradient clipping**\n",
    "    - **Gradient clipping** is a technique to prevent **exploding gradients** in very deep networks, a pre-determined gradient threshold be introduced, and then gradients norms that exceed this threshold are scaled down to match the norm.\n",
    "    - we can represent all the gradients as a vector and clipping is formulated as -\n",
    "    \n",
    "    $$\\text{G}_\\text{new} = \\bigg[ \\frac{\\text{G}}{||\\text{G}||_2} \\bigg] \\tau$$\n",
    "    \n",
    "    - where\n",
    "        - $\\text{G}$ - normal gradients\n",
    "        - $\\text{G}_\\text{new}$ - new gradients\n",
    "        - $||\\text{G}||_2$ - $\\text{L}_2$ - norm $\\implies \\sqrt{\\text{G}_1^2 + \\text{G}_2^2 + \\text{G}_3^2 + \\dots + \\text{G}_n^2}$\n",
    "        - $\\tau$ - threshold\n",
    "    \n",
    "    <img src=\"https://images.deepai.org/glossary-terms/f7ae7206ff0446979c407c78325e5753/gradclip.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax and Cross-Entrpy for Multi-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic regression is mainly used for binary classification.\n",
    "    - given $D = \\{x_i, y_i\\}; y_i \\in \\{0, 1\\}$\n",
    "    - for multi-class classification we use `one-vs-rest` method\n",
    "\n",
    "* But, with a slight tweaks in the mathematical formulation, we can extend logistic regression for multi-class classification task. Thus, we get **Softmax Classifier**.\n",
    "    - given $D = \\{x_i, y_i\\}; y_i \\in \\{1, 2, 3, \\dots, k\\}$\n",
    "\n",
    "* Multi-Classification using Softmax classifier.\n",
    "\n",
    "    <img src=\"https://deepnotes.io/public/images/softmax.png\">\n",
    "\n",
    "* Softmax minimizes the multi-class (also defined as cross-entropy) logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to train MLP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Preprocessing - Data Normalization.\n",
    "\n",
    "* Weight Initialization - \n",
    "    - Xavier or Glorot for sigmoid or tanh\n",
    "    - He for ReLU\n",
    "    - Gaussian distribution with a small variance\n",
    "\n",
    "* Choose the right activation function.\n",
    "    - ReLU\n",
    "\n",
    "* Try to batch normalization especially for the layers closer to the outer layer.\n",
    "    - dropout for regularization\n",
    "\n",
    "* Adam optimizer for faster convergence.\n",
    "\n",
    "* Hyperparameters, Architecture (# layers, # neurons), Dropout rate\n",
    "\n",
    "* Loss function.\n",
    "    - logloss - binary\n",
    "    - multi-class logloss - multiple classes\n",
    "    - squared loss - regression\n",
    "\n",
    "* Gradient Monitoring and Clipping.\n",
    "\n",
    "* Plot of epoch vs train loss and test loss.\n",
    "    - should show convergence to 0\n",
    "\n",
    "* Overfitting avoidance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto Encoder\n",
    "\n",
    "a neural network which performs dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given $D = \\{x_i\\}_{i=1}^n$ where $x_i \\in R^d$, the task to is to get $D^1 = \\{x_i^1\\}_{i=1}^n$ where $x_i^1 \\in R^{d^1}$ such that $d^1 < d$.\n",
    "    - we try to preserve the points from higher dimensional space to lower dimensional space\n",
    "    \n",
    "    $$\\text{expanded original data} \\ (x_i) \\implies \\text{compresssion (hidden layer auto-encoder)} \\implies \\text{expanded new data} \\ (\\hat{x_i})$$\n",
    "    \n",
    "    $$x_i \\sim \\hat{x_i} \\implies L (\\text{loss}) = 0$$\n",
    "    \n",
    "    $$L = || x_i - \\hat{x_i} ||^2$$\n",
    "\n",
    "    <img src=\"http://ufldl.stanford.edu/tutorial/images/Autoencoder636.png\">\n",
    "\n",
    "**Denoising Autoencoder (DAE)**\n",
    "\n",
    "* Denoising autoencoders (DAE) try to achieve a good representation by changing the reconstruction criterion.\n",
    "\n",
    "* Indeed, DAEs take a partially corrupted input and are trained to recover the original undistorted input. In practice, the objective of denoising autoencoders is that of cleaning the corrupted input, or denoising. Two assumptions are inherent to this approach:\n",
    "\n",
    "    - higher level representations are relatively stable and robust to the corruption of the input\n",
    "    - to perform denoising well, the model needs to extract features that capture useful structure in the input distribution\n",
    "\n",
    "    <img src=\"https://pbs.twimg.com/media/EeuLgrYUwAAF4fL.jpg\">\n",
    "\n",
    "**Credits** - Images from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given a sentence, we can choose any word as `focus`. The words around the `focus` word are called the `context` words.\n",
    "\n",
    "* `Context` words are very useful in understanding the `focus` word.\n",
    "\n",
    "* There are two algorithms -\n",
    "    - CBoW - Continuous Bag of Words\n",
    "    - Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's say we have a vocabulary of words (something like a dictionary) such as $V$.\n",
    "\n",
    "* Let's also say that $v$ is the length of the vocabulary.\n",
    "    - number of words that are present in the dictionary\n",
    "\n",
    "* We can represent each word in the form of one-hot-encoding.\n",
    "    - binary vector of $v$ dimensions\n",
    "    - $w_i \\in R^v$\n",
    "\n",
    "* **Core Idea**\n",
    "    - given the `context` words, can we predict the `focus` word ($v$ dimensional binary vector)\n",
    "    \n",
    "    <img src=\"https://i.stack.imgur.com/FWO7L.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skipgram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's say we have a vocabulary of words (something like a dictionary) such as $V$.\n",
    "\n",
    "* Let's also say that $v$ is the length of the vocabulary.\n",
    "    - number of words that are present in the dictionary\n",
    "\n",
    "* We can represent each word in the form of one-hot-encoding.\n",
    "    - binary vector of $v$ dimensions\n",
    "    - $w_i \\in R^v$\n",
    "\n",
    "* **Core Idea**\n",
    "    - given the `focus` word, can we predict the `context` words ($v$ dimensional binary vectors)\n",
    "    \n",
    "    <img src=\"https://mysaranshblog.files.wordpress.com/2016/11/igsue.png\">\n",
    "\n",
    "> Skipgram is computationally expensive.\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec - Algorithmic Optimizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For CBoW and skipgrams, there are millions of weights to be trained. This can take much time (forever).\n",
    "\n",
    "* **Algorithmic approaches**\n",
    "    - hierarchical softmax → modify the $V$ softmax activation functions to make it optimal\n",
    "    \n",
    "    ![hierarchical](https://user-images.githubusercontent.com/63333753/139835320-27fe8427-7e86-48c2-9a9b-9e5bf90631ff.png)\n",
    "        \n",
    "        - goes with appraoch of divide and conquer rules\n",
    "        - at the end, we only require $\\log_2{(V)}$ activation functions\n",
    "    \n",
    "    - negative sampling\n",
    "        - statistics based technique\n",
    "        - simply the idea that we only update a sample of output words per iteration\n",
    "        - the target output word should be kept in the sample and gets updated, and we add to this a few (non-target) words as negative samples\n",
    "        \n",
    "        $$P(w_i) = 1 - \\sqrt{\\frac{\\tau}{\\text{freq}(w_i)}}; \\tau \\rightarrow \\text{threshold} \\ (10^{-5})$$\n",
    "\n",
    "> A probabilistic distribution is needed for the sampling process, and it can be arbitrarily chosen. One can determine a good distribution empirically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
