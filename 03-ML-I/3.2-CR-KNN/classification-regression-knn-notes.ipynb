{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classification in machine learning and statistics is a supervised learning approach in which the computer program learns from the data given to it and make new observations or classifications.\n",
    "\n",
    "* The computer is presented with both example inputs and their respective outputs. The algorithm learns a general rule to map the inputs to the outputs.\n",
    "\n",
    "**Two Stages**\n",
    "\n",
    "* **Training** - We show the model a set of inputs along with the respective outputs. The task of the model is to learn by mapping the inputs and outputs.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/msameeruddin/Data-Analysis-Python/main/7_DA_Machine_Learning/training.png\">\n",
    "\n",
    "* **Testing/Evaluation** - We show the model a set of new inputs without the respective outputs. The aim of the model is to predict based on the learning it had undergone.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/msameeruddin/Data-Analysis-Python/main/7_DA_Machine_Learning/testing.png\">\n",
    "\n",
    "The model predicts the category based on the previous training or learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In any dataset $x_i$ represents the input and $y_i$ represents the class label. Let's say we have two class labels (+ve & -ve), we can have labels as (0, 1).\n",
    "\n",
    "* It is important to convert textual data into vectors for mathematically mapping inputs with respect to class labels.\n",
    "\n",
    "$$D_n = \\big\\{(x_i, y_i)_{i = 1}^n \\ | \\ x_i \\in \\rm I\\!R^d \\ \\& \\ y_i \\in (0, 1)\\big\\}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification VS Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* For classification, we shall have a finite set of class labels from which we predict the class of a new data.\n",
    "    - Blinary Classification - {0, 1}\n",
    "    - Multi Classification (MNIST) - {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
    "* For regression, the output values are generally continuous values (completely different from finite set of classes}\n",
    "    - Predicting the height (inches) based on the weight of the person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let us have data set that is plotted (visualized).\n",
    "\n",
    "![data_set-knn](https://user-images.githubusercontent.com/63333753/115667178-2acb7a00-a363-11eb-94cd-5770228fd3b6.png)\n",
    "\n",
    "* Let's introduce a new query point ($x_q$) plotted along with the data (magenta color).\n",
    "\n",
    "![qp-introduced](https://user-images.githubusercontent.com/63333753/115667817-e55b7c80-a363-11eb-995d-86857e3c31ca.png)\n",
    "\n",
    "* How do we determine which category $x_q$ belongs to?\n",
    "\n",
    "    - Find $k$ (=3) nearest points to $x_q$ in the data set.\n",
    "    - Apply voting system and take majority value of the $k$ points that they belong.\n",
    "        * Since $k = 3$, we shall have points as $\\{x_1, x_2, x_3 \\}$.\n",
    "        * Find the respective classes of points. Let's have $\\{y_1, y_2, y_3 \\}$ as classes and class values as $\\{\\text{positive}, \\text{positive}, \\text{negative} \\}$.\n",
    "        * The majority is $\\text{positive}$, hence $x_q$ belongs to positive categeory (blue).\n",
    "    \n",
    "    - **Note** - It is always recommended to consider $k$ to be an odd number.\n",
    "\n",
    "![geometric-intuit-knn](https://user-images.githubusercontent.com/63333753/115669067-8139b800-a365-11eb-93f8-f655e9b4d5df.png)\n",
    "\n",
    "This is the geometric intuition of KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distances are measured between two points whereas norms are measured between two vectors in $d$ dimensional space.\n",
    "\n",
    "* **Euclidean Distance** between two points\n",
    "    - $x_1 - (x_{11}, x_{12}) \\ \\text{and} \\ x_2 - (x_{21}, x_{22}) \\ \\text{is} \\ \\rightarrow ||x_1 - x_2|| = \\sqrt{(x_{21} - x_{11})^2 + (x_{22} - x_{12})^2}$\n",
    "    - We can also represent it in the form of $||x_1 - x_2||_2$\n",
    "    - To simplify, $$||x_1 - x_2||_2 \\rightarrow \\bigg[\\sum_{i = 1}^d \\big(x_{1i} - x_{2i}\\big)^2 \\bigg]^{\\frac{1}{2}} \\implies L_2 \\ \\text{Norm}$$\n",
    "\n",
    "* **Manhattan Distance** between two points\n",
    "    - $x_1 - (x_{11}, x_{12}, \\dots, x_{1d}) \\ \\text{and} \\ x_2 - (x_{21}, x_{22}, \\dots, x_{2d}) \\ \\text{is} \\ \\rightarrow ||x_1 - x_2|| = \\sum_{i = i}^d \\big| x_{1i} - x_{2i} \\big|$\n",
    "    - We can also represent it in the form of $||x_1 - x_2||_1 \\implies L_1 \\ \\text{Norm}$\n",
    "\n",
    "* **Minkowski Distance** between two points\n",
    "    - $x_1 - (x_{11}, x_{12}, \\dots, x_{1d}) \\ \\text{and} \\ x_2 - (x_{21}, x_{22}, \\dots, x_{2d}) \\ \\text{is} \\ \\rightarrow ||x_1 - x_2|| = \\bigg[\\sum_{i = 1}^d \\big|x_{1i} - x_{2i}\\big|^p \\bigg]^{\\frac{1}{p}}$\n",
    "    - We can also represent it in the form of $||x_1 - x_2||_p \\implies L_p \\ \\text{Norm}$\n",
    "    - If $(p = 1) \\rightarrow$ Manhattan Distance\n",
    "    - If $(p = 2) \\rightarrow$ Euclidean Distance\n",
    "    - $p$ should be always $> 0$\n",
    "\n",
    "* **Hamming Distance** is measured between two vectors\n",
    "    - Let \n",
    "    - $x_1$ be $[0, 1, 1, 0, 1, 0, 0]$ and \n",
    "    - $x_2$ be $[1, 0, 1, 0, 1, 0, 1]$\n",
    "    - Hamming distance is the number of locations or dimensions where binary vectors differ.\n",
    "    - $\\text{Ham}(x_1, x_2) = 3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DistanceMeasures():\n",
    "    def __init__(self, point1, point2):\n",
    "        self.point1 = np.array(point1)\n",
    "        self.point2 = np.array(point2)\n",
    "        self.flag = True if (len(self.point1) == len(self.point2)) else False\n",
    "    \n",
    "    def euclidean_measure(self):\n",
    "        if self.flag:\n",
    "            dist = np.linalg.norm(self.point1 - self.point2)\n",
    "            return round(dist, 4)\n",
    "        return None\n",
    "    \n",
    "    def manhattan_measure(self):\n",
    "        if self.flag:\n",
    "            dist = np.abs(self.point1 - self.point2).sum()\n",
    "            return round(dist, 4)\n",
    "        return None\n",
    "    \n",
    "    def minkowski_measure(self, p):\n",
    "        if not self.flag and (p <= 0):\n",
    "            return None\n",
    "        \n",
    "        if (p == 1):\n",
    "            return self.manhattan_measure()\n",
    "        elif (p == 2):\n",
    "            return self.euclidean_measure()\n",
    "        \n",
    "        dist = np.sum(np.abs(self.point1 - self.point2) ** p) ** (1 / p)\n",
    "        return round(dist, 4)\n",
    "    \n",
    "    def hamming_measure(self):\n",
    "        if self.flag:\n",
    "            return np.sum(self.point1 != self.point2)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Euclidean \t → 3.7417\n",
      "Manhattan \t → 6\n",
      "Minkowski \t → 3.3019\n",
      "Hamming \t → 3\n"
     ]
    }
   ],
   "source": [
    "p1 = [1, 2, 3, 4, 5]\n",
    "p2 = [4, 3, 1, 4, 5]\n",
    "\n",
    "dist = DistanceMeasures(point1=p1, point2=p2)\n",
    "\n",
    "print(\"Euclidean \\t →\", dist.euclidean_measure())\n",
    "print(\"Manhattan \\t →\", dist.manhattan_measure())\n",
    "print(\"Minkowski \\t →\", dist.minkowski_measure(p=3))\n",
    "print(\"Hamming \\t →\", dist.hamming_measure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship b/w Cosine Similarity and Cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study - https://cmry.github.io/notes/euclidean-v-cosine\n",
    "\n",
    "* The relationship between these two are inverse. If distance increases then similarity decreases and vice-versa.\n",
    "\n",
    "**General Notation**\n",
    "\n",
    "$$1 - \\text{cosine-sim}(x_1, x_2) = \\text{cosine-dis}(x_1, x_2)$$\n",
    "\n",
    "where\n",
    "\n",
    "* $x_1 = [x_{11}, x_{12}, \\dots, x_{1d}]$\n",
    "* $x_1 = [x_{21}, x_{22}, \\dots, x_{2d}]$\n",
    "\n",
    "$$\\text{cosine-sim}(x_1, x_2) = \\frac{x_1.x_2}{||x_1||_2 ||x_2||_2} = \\frac{x_1.x_2}{\\sqrt{x_1.x_1} \\sqrt{x_2.x_2}}$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* **Training error** is the error that you get when you run the trained model back on the training data.\n",
    "    - Remember that this data has already been used to train the model and this necessarily doesn't mean that the model once trained will accurately perform when applied back on the training data itself.\n",
    "\n",
    "* **Test error** is the error when you get when you run the trained model on a set of data that it has previously never been exposed to.\n",
    "    - This data is often used to measure the accuracy of the model before it is shipped to production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test / Evaluation - Time & Space Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Study - https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/\n",
    "\n",
    "The below is a simple `pseudo` code to determine time and space complexity for KNN.\n",
    "\n",
    "$x_q \\rightarrow y_q$\n",
    "\n",
    "**Inputs** - $D_{Train}$, $k$, $x_q \\in \\rm I\\!R^d$\n",
    "\n",
    "**Ouput** - $y_q$\n",
    "\n",
    "**Algo**\n",
    "\n",
    "```python\n",
    "KNNPts = []\n",
    "for each_xi in D_Train: # O(n)\n",
    "    di = compute_distance(each_xi, xq) # O(d)\n",
    "    # keep the smallest distances ==> (each_xi, yi, di)\n",
    "    KNNPts.append(s_each_xi, s_yi, s_di) # O(1)\n",
    "    \n",
    "    # Total time complexity → O(nd)\n",
    "\n",
    "cnt_pos = 0; cnt_neg = 0\n",
    "\n",
    "# O(k)\n",
    "for each_pair in KNNPts:\n",
    "    # O(1)\n",
    "    if each_pair[1] > 0:\n",
    "        cnt_pos += 1\n",
    "    else:\n",
    "        cnt_neg += 1\n",
    "\n",
    "# O(1)\n",
    "if cnt_pos > cnt_neg:\n",
    "    return yq = 1 # +ve\n",
    "return yq = 0 # -ve\n",
    "\n",
    "# Overall time complexity is O(nd) + O(1) + O(1) = O(nd)\n",
    "```\n",
    "\n",
    "* Time Complexity → $O(nd)$\n",
    "* Space Complexity → $O(nd)$\n",
    "\n",
    "**Can we reduce the time and space complexities?**\n",
    "\n",
    "* We shall use the state of the art algorithms such as -\n",
    "\n",
    "    * KD-Tree (data structure)\n",
    "    * LSH - Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision surface for KNN as `K` changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A (hyper) surface in a multidimensional state space that partitions the space into different regions.\n",
    "* Data lying on one side of a decision surface are defined as belonging to a different class from those lying on the other.\n",
    "\n",
    "![decision-surface](https://user-images.githubusercontent.com/63333753/116030754-ee0cc500-a679-11eb-972f-4c87e6e97062.png)\n",
    "\n",
    "* Decision surfaces help us understand what happens as `k` increases.\n",
    "* As `K` increases, the smoothness of the curve increases.\n",
    "* If `K` is exactly equal to `n`, all the new query points become the majority class no matter how deep inside the region the point is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Overfitting** - is an error that occurs in data modeling as a result of a particular function aligning too closely to a minimal set of data points.\n",
    "* **Regular fitting** - Model fitting is a measure of how well a machine learning model generalizes to similar data to that on which it was trained. A model that is well-fitted produces more accurate outcomes.\n",
    "* **Underfitting** - A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the underlying trend of the data. Underfitting destroys the accuracy of our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Study\n",
    "    * http://ethen8181.github.io/machine-learning/model_selection/model_selection.html\n",
    "    * https://www.ritchieng.com/machine-learning-cross-validation/\n",
    "\n",
    "<br>\n",
    "\n",
    "* Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n",
    "* The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation.\n",
    "\n",
    "**Generalization** - If an algorithm performs well for an unseen data point, it is known as generalization.\n",
    "\n",
    "<img src=\"http://ethen8181.github.io/machine-learning/model_selection/img/kfolds.png\">\n",
    "\n",
    "**Credits** - Image from Internet\n",
    "\n",
    "We divide the whole data set in to 3 parts -\n",
    "\n",
    "* Training - it is used to determine `NN`.\n",
    "* Validation - it is used to determine `K`.\n",
    "* Testing (unseen) - it is used to predict the class label for an unseen data.\n",
    "\n",
    "The time complexity for k-fold cross validation is $O(n*k*d)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the data - (Train, CV, Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $D_{\\text{Train}}$, $D_{\\text{CV}}$, and $D_{\\text{Test}}$ do not overlap perfectly (because we split the data randomly).\n",
    "\n",
    "* If there are many $\\text{+ve}$ or $\\text{-ve}$ points from $D_{\\text{Train}}$ in a region then it is highly likely to find many $\\text{+ve}$ or $\\text{-ve}$ points from $D_{\\text{CV}}$ in that region.\n",
    "\n",
    "* On the other hand, if there are very few $\\text{+ve}$ or $\\text{-ve}$ points in a region from $D_{\\text{Train}}$ then it is very unlikely to find $\\text{+ve}$ or $\\text{-ve}$ points from $D_{\\text{CV}}$ in that region. This represents that the data contain noisy values.\n",
    "\n",
    "![intuitive-viz](https://user-images.githubusercontent.com/63333753/116349082-dade2e80-a80c-11eb-8bed-8ba86991f631.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train error, Cross Validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Training Error:** We get the by calculating the classification error of a model on the same data the model was trained on.\n",
    "\n",
    "* **Cross Validation Error:** We get by calculating the classification error of a model on the cross validation data using the trained data for training.\n",
    "\n",
    "* **Test Error:** We usually get the test error after getting the accuracy i.e., $(1 - \\text{accuracy})$. We generally report this in order to see how best the model performs.\n",
    "\n",
    "![ttcv-errors](https://user-images.githubusercontent.com/63333753/116353894-314f6b00-a815-11eb-8bb0-0f053f7311a3.png)\n",
    "\n",
    "**Note:** When Training Error and CV Error is more then it is known as **Underfitting**. Similarly, when Training Error is less and CV Error is more, then it is known as **Overfitting**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time based Splitting (TBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a time/date feature in the data set then it is better to use this method.\n",
    "\n",
    "* First sort the data based on time feature.\n",
    "* Select first 60% data for training data.\n",
    "* Next 20% data for cross validation.\n",
    "* Further 20% data for test data.\n",
    "\n",
    "With time, many things change (reviews, products) so when time column is available, it is good to choose TBS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some classification algorithms can be extended to do regression analysis. A simple notation on how we can do is below.\n",
    "\n",
    "* Given $x_q$, find k-nearest neighbors\n",
    "    - $(x_1, y_1), (x_2, y_2), (x_3, y_3), \\dots, (x_k, y_k)$\n",
    "* **Classification** → workds based on majority votes.\n",
    "* **Regression** → It is sure that -\n",
    "    - $y_q \\in \\{y_1, y_2, y_3, \\dots, y_k\\} \\rightarrow \\rm I\\!R$\n",
    "    - $y_q \\rightarrow \\text{Mean}({y_i})_{i=1}^k$\n",
    "    - $y_q \\rightarrow \\text{Median}({y_i})_{i=1}^k$\n",
    "    - We shall not take mode into consideration as it may lead to the same technique such as majority vote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is an extension to a regular KNN implementation.\n",
    "* A weight is added to each distance that is computed.\n",
    "* More weight if less distance and vice-versa.\n",
    "* Sum up all the weights per class label and choose the one which is the largest of all.\n",
    "\n",
    "**One way** to determine how much weight should be given is $w_i = \\frac{1}{d_i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kd Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine the following image is your data visualized\n",
    "\n",
    "![kd-tree-points](https://user-images.githubusercontent.com/63333753/116850618-94267500-ac0e-11eb-9f29-e1f02c40d7b6.png)\n",
    "\n",
    "To construct a `Kd` Tree, we must follow the below steps\n",
    "\n",
    "* Split the data with the **median** by projecting the points on `X` axis.\n",
    "\n",
    "![kd-tree-step1](https://user-images.githubusercontent.com/63333753/116852540-1ebca380-ac12-11eb-9e99-8b84934d1470.png)\n",
    "\n",
    "* Alternate between axes and repeat the step 1.\n",
    "\n",
    "![kd-tree-step2](https://user-images.githubusercontent.com/63333753/116858391-e9b54e80-ac1b-11eb-96ec-c771154851e4.png)\n",
    "\n",
    "* We go through each axis one after the other as done in the above diagram in order to construct a tree till we reach leaf nodes.\n",
    "\n",
    "\n",
    "With `Kd` Tree, we are breaking up the space using axis parallel lines or planes into rectanlges/cuboids/hypercuboids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of `Kd` Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When `d` is small - {2, 3, 4, 5} → `Kd` tree works efficiently.\n",
    "* When `d` is `>=` 10 → the time complexity increases dramatically.\n",
    "* `Kd` tree was developed first to solve some problems related to computer graphics. It has no proper applications in Machine Learning where we deal with so many dimensions of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Uses the concept of hashing and hashtable to find the nearest neighbors.\n",
    "* Stores the data in a `key` and `value` pair (similar to dictionary).\n",
    "* $\\text{hash}(x_q) \\rightarrow$ used to find the nearest neighbor for a new query point. The time complexity for the same is $O(1)$ (less time).\n",
    "\n",
    "![lsh-concept](https://user-images.githubusercontent.com/63333753/116969053-40ca2a80-acd3-11eb-9139-346eed4fe5b5.png)\n",
    "\n",
    "* This technique works well even the dimension ($d$) is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSH  for Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps for locality sensitive hashing:\n",
    "\n",
    "1. First make ‘m’ hyperplanes to split into regions and create slices such that cluster of points lie in a particular slice and be called their neighbourhood. Typically $m = \\log(n)$\n",
    "\n",
    "2. Next for each point create a vector (also called hash function) by $W_1^T \\ \\text{point}$. If it is greater than 0 , it lies on the same side of that hyperplane else other side. Based on that create a vector of m size. \n",
    "    - For eg the vector can be [1,-1,-1] denoting point $x$ lies on same side of normal to hyperplane 1, opposite side to normal of hyperplane 2 and 3. Now this vector serves as key to the hash table and all the points with the same key or vector representation will go in the same bucket as they have similar vector representation denoting they lie in the neighbourhood of each other.\n",
    "\n",
    "![lsh-cosine](https://user-images.githubusercontent.com/63333753/116975496-5d6b6000-acdd-11eb-9d3a-8c16129d88cc.png)\n",
    "\n",
    "| Key | local points |\n",
    "| --- | --- |\n",
    "| [1 1 1] | {x1, x2, x3, x4, x5 } |\n",
    "| [1 -1 -1] | {x6, x7, x8, x9 } |\n",
    "\n",
    "3. Now it may happen that two points which are very close fall on different slice due to placing of hyperplane and hence not considered as nearest neighbour. To resolve this problem, create `l` hash tables (`l` is typically small). In other words repeat **step 2** `l` times thus creating l hash tables and m random planes `l` times. So when a query point comes, compute the hash function each of the `l` times and get its neighbours from each of bucket. Union them and find the nearest neighbours from the list. So basically in each `l` iterations create m hyperplanes and hence region splitting will be different thus vector representation or hash function of the same query point will be different in each of the representations. Thus the hash table will be different as points which lied on the same region in previous iteration might lie in a different region in this iteration and vice versa due to different placement of hyperplanes.\n",
    "\n",
    "**Time and Space Complexity**\n",
    "\n",
    "* Time complexity is $O(m*d*l)$ for each query point. And for creating the hash table $O(m*d*l*n)$ which is one time only\n",
    "* Space complexity is $O(n)$\n",
    "\n",
    "<br>\n",
    "\n",
    "> **LSH** is extensively used in Computer Vision areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Class Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In probabilistic approach instead of giving deterministic value, we want to establish certainty by giving probabilities that is helpful sometimes like in medical applications, where we also want to state how confident you are about our decision of one class or another wherein the probabilistic class-labels are very useful. You can combine probabilistic approach with weighted knn as follows:\n",
    "\n",
    "1. Let $d_1$, $d_2$, and $d_3$ be the distances to the `3 NNs`. Here, let $d_1$ be the distance to the `+ve` labeled points and $d_2$ and $d_3$ be the distances to `-ve` labeled points. Let $\\frac{1}{d_1} = w_1$, $\\frac{1}{d_2} = w_2$, $\\frac{1}{d_3} = w_3$ where $w_i$ is the weights.\n",
    "\n",
    "2. $P(x_q = \\text{+ve}) = \\frac{w_1}{(w_1+w_2+w_3)}$ and $P(x_q = \\text{-ve}) = \\frac{(w_2+w_3)}{(w_1+w_2+w_3)}$. Now use these ratios to obtain the probability values to decide the class label.\n",
    "\n",
    "In a nutshell, instead of using the count of 1 to compute the probabilistic scores, we are using the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interview Questions\n",
    "\n",
    "* https://www.analyticsvidhya.com/blog/2017/09/30-questions-test-k-nearest-neighbors-algorithm/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "304.475px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
