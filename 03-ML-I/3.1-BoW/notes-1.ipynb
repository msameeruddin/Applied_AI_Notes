{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textblob as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blobber', 'PACKAGE_DIR', 'Sentence', 'TextBlob', 'Word', 'WordList', '__all__', '__author__', '__builtins__', '__cached__', '__doc__', '__file__', '__license__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_text', 'base', 'blob', 'compat', 'decorators', 'en', 'exceptions', 'inflect', 'mixins', 'np_extractors', 'os', 'parsers', 'sentiments', 'taggers', 'tokenizers', 'translate', 'utils']\n"
     ]
    }
   ],
   "source": [
    "print(dir(tb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before sparse matrix computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Convert the text into lower case\n",
    "* Remove all stopwords\n",
    "* Apply Stemming (to get the base word)\n",
    "* Check for semantics\n",
    "\n",
    "> Lemmatization, Tokenization, Semantics, Stemming are all important for text-preprocessing.\n",
    "\n",
    "* Lemmatization gives the actual verb form of the word\n",
    "    - 'go', 'went', 'gone' → go\n",
    "* Tokenization gives the separated (splits) the entire text into words\n",
    "    - 'I am goind' → 'I', 'am', 'going'\n",
    "* Semantics gives the sense between two or multiple words\n",
    "    - 'tasty' & 'delicious' → both are same\n",
    "* Stemming gives the base word among the words provided"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW\n",
    "\n",
    "link - https://www.youtube.com/watch?v=IRKDrrzh4dE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BoW → Bag of Words, used to convert a text into numerical vectors\n",
    "    - Uni-grams\n",
    "    - Bi-grams\n",
    "    - Tri-grams\n",
    "    - n-grams\n",
    "    - mainly used text pre-processing\n",
    "\n",
    "> **Example**\n",
    "\n",
    "* text 1 → Rome is not built in a day.\n",
    "    - Here there are no repeated words. So \n",
    "    - unigrams → not,  in, day, is, a, built, Rome\n",
    "    - bigrams → Rome is, is not, not built, built in, in a, a day.\n",
    "    - trigrams → Rome is not, is not built, not built in, built in a, in a day.\n",
    "\n",
    "* Here as there is no repetetion of words, \n",
    "    - number of unigrams > number of bigrams > number of trigrams.\n",
    "\n",
    "---\n",
    "\n",
    "* text 2 → horse is a horse, of course, of course. accept  it\n",
    "    - Here there are repeated words\n",
    "    - unigrams → horse, of, course, a, is, it, accept\n",
    "    - bigrams → horse is, is a , a horse, horse of, of course, course of, course accept, accept it\n",
    "    - trigrams → horse is a, is a horse, a horse of, horse of course, of course of, course of course, of course accept, course accept it\n",
    "* Here there is repetetion of words in the given sentence. Hence\n",
    "    - number of unigrams < number of bigrams < number of trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine, we have `N` reviews or documents and each review is a combination of words\n",
    "\n",
    "$$r_1 \\rightarrow w_1, w_2, w_3, w_2, w_5$$\n",
    "\n",
    "and\n",
    "\n",
    "$$r_2 \\rightarrow w_1, w_3, w_4, w_5, w_6, w_2$$\n",
    "\n",
    "then\n",
    "\n",
    "* TF → Term Frequency\n",
    "\n",
    "    - $$TF(w_i, r_j) = \\frac{w_i \\ \\text{count}}{\\text{total words in} \\ r_j} \\implies 0 \\leq TF(w_i, r_j) \\leq 1$$\n",
    "    \n",
    "    - $TF(w_2, r_1) = \\frac{2}{5}$\n",
    "    - $TF(w_2, r_2) = \\frac{1}{6}$\n",
    "\n",
    "* IDF → Inverse Document Frequency\n",
    "\n",
    "    - $$IDF(w_i, D_c) = \\log \\bigg[\\frac{N}{n_i} \\bigg] \\implies (n_i \\leq N), \\bigg(\\frac{N}{n_i} \\geq 1 \\bigg), \\ \\text{and} \\ \\bigg(\\log \\bigg[\\frac{N}{n_i} \\bigg] \\geq 0 \\bigg)$$\n",
    "    - $$IDF(w_i, D_c) \\geq 0$$\n",
    "    - where $N$ is total number of documents and $n_i$ is the number of documents that contain the word $w_i$\n",
    "    \n",
    "    - We compute IDF of each word with respect to the entire Data Corpus (includes all documents or reviews).\n",
    "    - Let $D_c$ is my corpus data where $D_c = \\{r_1, r_2, r_3, \\dots, r_N \\}$\n",
    "    - And let $w_1$ occurs in $r_1, r_3, \\text{and}, r_6$ then\n",
    "    - $IDF(w_1, D_c) = \\log \\big[\\frac{N}{3} \\big]$\n",
    "\n",
    "---\n",
    "\n",
    "Finally\n",
    "\n",
    "$$\\text{TFIDF} = TF(w_i, r_j) * IDF(w_i, D_c) \\implies \\frac{\\text{word count in} \\ r_j}{\\text{total words in} \\ r_j} * \\log \\bigg[\\frac{\\text{total docs}}{\\text{count of docs where word is contained}} \\bigg]$$\n",
    "\n",
    "* more importance to rarer words\n",
    "* more importance if a word is frequent in a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word 2 Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a nice way of converting a word into vector. It holds the semantics between words and considered to be one of the state of the art techniques.\n",
    "\n",
    "* The idea behind Word2Vec is pretty simple. We're making an assumption that the meaning of a word can be inferred by the company it keeps. This is analogous to the saying, \"show me your friends, and I’ll tell who you are\".\n",
    "\n",
    "> Helpful link → [here](https://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.Xr_Dp0QzY_4)\n",
    "\n",
    "> Simulation → https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence 2 VeC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Word 2 Vec` recturns a vector represenation of a particular word. Unlike TF-IDF and bag of words, we cannot get the vecotr form of a sentence or a document.\n",
    "\n",
    "In order to obtain it, we follow techniques such as -\n",
    "\n",
    "* Avg Word 2 Vec → Simple take the average of vecotrs obtained from `Word 2 Vec` method\n",
    "* TF-IDF Word 2 Vec → weighted average (considereing the TF-IDF and each vector form word after `Word 2 Vec` method)\n",
    "\n",
    "The above work very well but not always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
