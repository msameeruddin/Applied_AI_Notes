{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Essence of Calculus\n",
    "\n",
    "* Refer to â†’ https://bit.ly/3iuFNlO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Single variable differentiation\n",
    "* Vector differentiation\n",
    "* Maxima and Minima\n",
    "* Integration\n",
    "* Differential equations\n",
    "\n",
    "---\n",
    "\n",
    "**Single variable differentiation**\n",
    "\n",
    "![diff-1](https://user-images.githubusercontent.com/63333753/121473277-0d6c7100-c9e0-11eb-897a-d31a211bb91b.png)\n",
    "\n",
    "In the above figure, $x$ is a scalar. We define $y$ as the function of $x$ such as -\n",
    "\n",
    "$$y = f(x) \\rightarrow (1)$$\n",
    "\n",
    "Differentiating $y$ w.r.t $x$, we get\n",
    "\n",
    "$$(1) \\implies \\frac{dy}{dx} = \\frac{df}{dx} = y^1 = f^1 \\rightarrow (2)$$\n",
    "\n",
    "Here, $\\frac{dy}{dx}$ simply means that how much of $y$ changes when $x$ changes. The rate of change of $y$ w.r.t $x$.\n",
    "\n",
    "$$(2) \\implies \\frac{y_2 - y_1}{x_2 - x_1} = \\frac{\\Delta y}{\\Delta x} = \\frac{\\sin \\theta}{\\cos \\theta} = \\tan \\theta$$\n",
    "\n",
    "To formulate the same mathematically, we get\n",
    "\n",
    "$$\\frac{dy}{dx} = \\lim_{\\Delta x \\rightarrow 0} \\frac{\\Delta y}{\\Delta x}$$\n",
    "\n",
    "The derivative of $y$ w.r.t $x$ is the rate of change of $y$ w.r.t $x$ as the change in $x$ is very small (tending to $0$). Tangent is the hypotenuse that we get as $\\Delta x \\rightarrow 0$ and thus ($\\tan \\theta$) is $\\frac{dy}{dx}$.\n",
    "\n",
    "**Basic formulae**\n",
    "\n",
    "$$\\frac{d}{dx}x^n = nx^{n-1}; \\frac{d}{dx}x^2 = 2x$$\n",
    "\n",
    "$$\\frac{d}{dx}C = 0$$\n",
    "\n",
    "$$\\frac{d}{dx}Cx^n = Cnx^{n-1}$$\n",
    "\n",
    "$$\\frac{d}{dx} \\log x = \\frac{1}{x}$$\n",
    "\n",
    "$$\\frac{d}{dx}e^x = e^x$$\n",
    "\n",
    "$$\\frac{d}{dx}\\big[f(x) + g(x)\\big] = \\frac{d}{dx}f(x) + \\frac{d}{dx}g(x)$$\n",
    "\n",
    "$$\\frac{d}{dx}f\\big[g(x)\\big] = \\frac{d}{dg}f \\frac{d}{dx}g$$\n",
    "\n",
    "Let's take $f\\big[g(x)\\big] = (a - bx)^2$\n",
    "\n",
    "Now $\\frac{d}{dx}f\\big[g(x)\\big]$ is\n",
    "\n",
    "$\\implies \\frac{d}{dx}(a - bx)^2 = 2 (a - bx) \\frac{d}{dx} (-bx) = 2 (a - bx) (-b) = -2b (a - bx)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online tool - Derivative calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Wolframalpha - https://www.wolframalpha.com/\n",
    "* Derivative calculator - https://www.derivative-calculator.net/\n",
    "* SymPy - https://www.sympygamma.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maxima & Minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In mathematical analysis, the maxima and minima (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function.\n",
    "\n",
    "![diff-2](https://user-images.githubusercontent.com/63333753/121493559-92ae5080-c9f5-11eb-931c-bec0611f492f.png)\n",
    "\n",
    "* The above graph does not have **global minima** and **global maxima**.\n",
    "* The slope at minima and maxima will be 0.\n",
    "* Parabola does not have maxima, it has only minima.\n",
    "* Inverse parabola does not have minima, it has only maxima.\n",
    "* There can be cases where there will not be any maxima or minima.\n",
    "\n",
    "![diff-3](https://user-images.githubusercontent.com/63333753/121506549-71ebf800-ca01-11eb-92af-2bf5a84202a2.png)\n",
    "\n",
    "* The minimum of all the minima is called **global minima**.\n",
    "* The maximum of all the maxima is called **global maxima**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that \n",
    "\n",
    "$$y = f(x) = a^Tx = \\sum_{i = 1}^d a_ix_i = a_1x_1 + a_2x_2 + \\dots + a_dx_d$$\n",
    "\n",
    "where\n",
    "\n",
    "* $a \\ \\text{(column vector)} = [a_1, a_2, a_3, \\dots, a_d]$\n",
    "* $x = [x_1, x_2, x_3, \\dots, x_d]$\n",
    "\n",
    "Clearly, $a$ and $x$ are vectors. To differentiate this, we should vector calculs that follows\n",
    "\n",
    "$$\\implies \\nabla_x f = \\bigg[\\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\dots, \\frac{\\partial f}{\\partial x_d}\\bigg]$$\n",
    "\n",
    "$$\\implies \\nabla_x f = \\bigg[\\frac{\\partial}{\\partial x_1} a_1x_1, \\frac{\\partial}{\\partial x_2} a_2x_2, \\dots, \\frac{\\partial}{\\partial x_d} a_dx_d\\bigg]$$\n",
    "\n",
    "$$\\implies \\nabla_x f = [a_1, a_2, a_3, \\dots, a_d] \\ \\text{(column vector)}$$\n",
    "\n",
    "$$\\implies \\nabla_x (a^Tx) = a$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Iterative algorithm\n",
    "* The objective is to move closer to the optimal value iteratively. This has to be done by first picking a random value.\n",
    "* As the point mover closer to optimal value, the slope reduces (in quadrant 1). Whereas in quandrant 2, the slope increases.\n",
    "\n",
    "---\n",
    "\n",
    "1. Pick an initial point $x_0$ at random\n",
    "\n",
    "![diff-4](https://user-images.githubusercontent.com/63333753/121680797-96b29f00-cad7-11eb-865d-c9685c8421f7.png)\n",
    "\n",
    "2. $x_1 = x_0 - r \\big[\\frac{df}{dx}\\big]_{x_0}$ - here $r$ is constant (step size). Let step size is 1.\n",
    "\n",
    "3. $x_2 = x_1 - r \\big[\\frac{df}{dx}\\big]_{x_1}$ - here $r$ is constant (step size). Let step size is 1.\n",
    "\n",
    "(As we continue, we reach to positions where the new point will be closer to the optimal value $x^*$)\n",
    "\n",
    "$$x_{i+1} = x_i - r \\bigg[\\frac{df}{dx}\\bigg]_{x_i}$$\n",
    "\n",
    "4. We continue doing this $k$ times, i.e., $x_k = x_{k-1} - r \\big[\\frac{df}{dx}\\big]_{x_{k-1}}$\n",
    "\n",
    "5. If $(x_{k+1} - x_k)$ is very small, then terminate the iterative process and declare $(x^* = x_k)$\n",
    "\n",
    "---\n",
    "\n",
    "* Gradient descent is an iterative algorithm with a simple update function.\n",
    "\n",
    "* If we are dealing with vectors then -\n",
    "\n",
    "$$x_{i+1} = x_i - r \\big[\\nabla_x f\\big]_{x_i}$$\n",
    "\n",
    "* Initially, the step size will be large. But as we proceed, the step size decreases drastically. It is because of the magic of differentiation.\n",
    "\n",
    "![diff-5](https://user-images.githubusercontent.com/63333753/121683320-d5962400-cada-11eb-8a15-5314f8065145.png)\n",
    "\n",
    "* Step size is also called as learning rate.\n",
    "\n",
    "* For every iteration, it is good to reduce the learning rate $(r)$. Otherwise, it will end up being an **oscillation problem**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The equation of linear regression in terms of vector is -\n",
    "\n",
    "$$w^* = argmin_w \\sum_{i=1}^n (y_i - w^Tx_i)^2$$\n",
    "\n",
    "Here\n",
    "\n",
    "* $x_i$ and $y_i$ are coming from training data, so they are taken as constants\n",
    "* $w$ is the one which needs to be found the optimal value\n",
    "\n",
    "Let $$f = \\sum_{i = 1}^n (y_i - w^Tx_i)^2 \\rightarrow (1)$$\n",
    "\n",
    "The problem that we are trying to solve is $\\nabla_w f = 0$\n",
    "\n",
    "When we apply vector calculus and differentiate $f$ w.t.x, we get -\n",
    "\n",
    "$$\\nabla_w f = \\sum_{i=1}^n \\big\\{-2x_i(y_i - w^Tx_i)\\big\\}$$\n",
    "\n",
    "1. Pick a random vector $w_o = [-, -, -, \\dots, -]$\n",
    "2. $w_1 = w_0 - r \\sum_{i=1}^n \\big\\{-2x_i(y_i - w^T_0x_i)\\big\\}$\n",
    "3. $w_2 = w_1 - r \\sum_{i=1}^n \\big\\{-2x_i(y_i - w^T_1x_i)\\big\\}$\n",
    "4. ...\n",
    "5. If $(w_{k+1} - w_k)$ is a vector of very small values then declare $w^* = w_k$\n",
    "\n",
    "If vector size is very large, then gradient descent algorithm can become slow. To avoid that, we have to use stochastic gradient descent algorithm techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent Algorithm (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SGD is the most important optimization algorithm in the whole of machine learning.\n",
    "\n",
    "* General linear regression optimation equation is -\n",
    "\n",
    "$$w_{j+1} = w_j - r \\sum_{i=1}^n \\big\\{-2x_i(y_i - w^T_jx_i)\\big\\}$$\n",
    "\n",
    "* SGD representation for the same problem is -\n",
    "\n",
    "$$w_{j+1} = w_j - r \\sum_{i=1}^k \\big\\{-2x_i(y_i - w^T_jx_i)\\big\\}; (1 < k \\leq n) \\ \\text{pick a random set of k points}$$\n",
    "\n",
    "* SGD uses probabilistic methods - randomization.\n",
    "\n",
    "* For every iteration, $k$ changes. It is because we select $k$ randomly. Here, $k$ is often called as the batch-size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
