{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linearly separable**\n",
    "\n",
    "* In Euclidean geometry, linear separability is a property of two sets of points. ... These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side.\n",
    "\n",
    "**Linearly non-separable**\n",
    "\n",
    "* A linearly nonseparable problem is a problem that, when represented as a pattern space (see above), requires more than one straight cut to separate all of the patterns of one type in the space from all of the patterns of another type.\n",
    "\n",
    "<!-- <img src=\"https://lh3.googleusercontent.com/proxy/n5B_7sOJ8TKSmAaeblWuEcn6Z_ImJAj5U1yq_ITsQhpwfFReM6gq9XYzC-4-GkWBRlJPwzZbdKlnLxtFFGcgi3kDTw2IR3T4BmcKb_R_9YH9xtpkJcYKziMHolAbEQ\">\n",
    "\n",
    "**Credit** - Image from Internet -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist.\n",
    "* It is a classification model that uses logistic function.\n",
    "* Given the data points, the task is to find a plane that can separate two classes.\n",
    "$$\\prod = w^Tx+ b$$\n",
    "    - where\n",
    "        - $\\prod$ is plane\n",
    "        - $w$ is normal to the plane ($\\prod$) $\\implies (||w|| = 1)$\n",
    "        - $b$ is intercept\n",
    "\n",
    "![logistic_reg](https://user-images.githubusercontent.com/63333753/120607946-48195b00-c46e-11eb-82ec-1e3d7833f745.png)\n",
    "\n",
    "Let's say we have classes where such as $y_i \\in \\{+1, -1\\}$. Let's also assume that the plane is passing through the origin $(b = 0)$.\n",
    "\n",
    "* $d_i$ is the distance between $x_i$ and the plane $\\prod$ which is equal to $\\frac{w^Tx_i}{||w||}$\n",
    "* $d_j$ is the distance between $x_j$ and the plane $\\prod$ which is equal to $\\frac{w^Tx_j}{||w||}$\n",
    "\n",
    "Here $w$ and $x_i$ are on the sample plane (direction) we will have\n",
    "\n",
    "* $d_i = w^Tx_i + b > 0 \\implies y_i = +1$\n",
    "* $d_j = w^Tx_j + b < 0 \\implies y_j = -1$\n",
    "\n",
    "> If $y_i*w^Tx_i$ and $y_j*w^Tx_j$ are greater than $0$, then it simply means that the classifier or model is correctly predicting.\n",
    "\n",
    "To break it down, the important task here is to find the optimum values of $w$ and $b$ that can minimize the error and maximizes $y_i*w^Tx_i$ and $y_j*w^Tx_j$ to be greater than $0$\n",
    "\n",
    "> optimal $w* = argmax \\sum_{i=1}^n y_iw^Tx_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance of sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Outliers can impact the classifier drastically. It can misclassify the points just because of one or more outliers.\n",
    "* To prevent this, sigmoid function is used.\n",
    "* The sigmoid function is also called as squashing function that reduces the value of any larger value.\n",
    "\n",
    "![sigmoid_func](https://user-images.githubusercontent.com/63333753/120631798-e7961800-c485-11eb-8422-180209c5a506.png)\n",
    "\n",
    "* The sigmoid function is often written as $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "* When we introduce this function to the orignal equation, we will get it as -\n",
    "\n",
    "$$w^* = argmax \\sum_{i=1}^n \\sigma(y_iw^Tx_i)$$\n",
    "\n",
    "* min($\\sigma$) is 0\n",
    "* max($\\sigma$) is 1\n",
    "\n",
    "* Finally we can write the equation of optimal w* as -\n",
    "\n",
    "$$w^* = argmax \\sum_{i=1}^n \\frac{1}{1 + \\exp(-y_iw^Tx_i)}$$\n",
    "\n",
    "* Sigmoid function is very easy to differentiate since it has probabilistic interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical formulation of an objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monotonic function**\n",
    "\n",
    "* The function $g(x)$ is said to be monotonic iff $x$ increases then $g(x)$ also increases, and vice-versa.\n",
    "    - Monotonically increasing → if $x$ increases then $g(x)$ increases\n",
    "    - Monotonically decreasing → if $x$ decreases then $g(x)$ decreases\n",
    "\n",
    "**Minima & Maxima**\n",
    "\n",
    "* In mathematical analysis, the `maxima` and `minima` (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema), or on the entire domain (the global or absolute extrema).\n",
    "\n",
    "<img src=\"https://datascienceintuition.files.wordpress.com/2017/12/local_global_maxmin.png\">\n",
    "\n",
    "**Credit** - Image from Internet\n",
    "\n",
    "Let's say we have an optimzation problem, i.e., $x^* = argmin(f(x))$ and let $f(x) = x^2$.\n",
    "\n",
    "* The $argmin(f(x)) \\implies argmin(x^2) = 0$\n",
    "\n",
    "![x-sqaure-graph](https://user-images.githubusercontent.com/63333753/120757597-60e94580-c52e-11eb-87d1-a8d96d0a5ea1.PNG)\n",
    "\n",
    "* From the above graph we can see that the local minima of $x^2$ is at $0$.\n",
    "\n",
    "If we introduce a monotonic function such as $g(x)$ upon $f(x)$ i.e., $x^1 = argmin(g(f(x)) \\implies argmin(g(x^2)$ and let's say that $g(x) = log(x)$.\n",
    "\n",
    "* We can claim that $x^* = x^1$, as from the below diagram.\n",
    "\n",
    "![log_x-square-graph](https://user-images.githubusercontent.com/63333753/120758215-2d5aeb00-c52f-11eb-8128-92bca7fc2c02.PNG)\n",
    "\n",
    "**Credit** - Images taken from Google\n",
    "\n",
    "Now, let $w^* = argmax \\sum_{i=1}^n \\frac{1}{1 + \\exp(-y_iw^Tx_i)} \\rightarrow (1)$\n",
    "\n",
    "Introduce monotonic function to $(1)$\n",
    "\n",
    "$\\implies w^* = argmax \\sum_{i=1}^n \\log \\bigg(\\frac{1}{1 + \\exp(-y_iw^Tx_i)} \\bigg)$\n",
    "\n",
    "**Note**: $\\log \\big(\\frac{1}{x}\\big) = -\\log(x)$\n",
    "\n",
    "$\\implies w^* = argmax \\sum_{i=1}^n - \\log \\big({1 + \\exp(-y_iw^Tx_i)} \\big)$\n",
    "\n",
    "**Note**: $argmax(-f(x)) = argmin(f(x))$\n",
    "\n",
    "$\\implies w^* = argmin \\sum_{i=1}^n \\log \\big({1 + \\exp(-y_iw^Tx_i)} \\big) \\rightarrow (2)$ where $y_i \\in \\{+1, -1\\}$\n",
    "\n",
    "The equation $(2)$ represents the optimization problem of logistic regression. This will not be impacted by outliers. If we try to negate $1$, then $\\log$ and $\\exp$ gets cancelled out and we end up remaining with the same problem without the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L2$ Regularization : Overfitting and Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w.k.t\n",
    "\n",
    "$$w^* = argmin \\sum_{i=1}^n \\log \\big({1 + \\exp(-y_iw^Tx_i)} \\big) \\rightarrow (1)$$\n",
    "\n",
    "Let $Z_i = y_iw^Tx_i$\n",
    "\n",
    "We can write $(1)$ as -\n",
    "\n",
    "$$(1) \\implies w^* = argmin \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) \\rightarrow (2)$$\n",
    "\n",
    "* The function value of $\\exp(-x)$ be always $\\geq 0$\n",
    "\n",
    "![exp_minus_x-graph](https://user-images.githubusercontent.com/63333753/120772600-1a034c00-c53e-11eb-9cdd-e2741998df27.PNG)\n",
    "\n",
    "* w.k.t $(\\log(1) = 0)$ and $(\\log(1 + \\delta) \\geq \\log(1))$ if  $(\\delta \\geq 0)$\n",
    "* So, mathematically $(2)$ is always $\\geq 0$, and thus the minimal value will be $0$\n",
    "\n",
    "Now, for what values of $Z_i$ the function value of $\\exp(-Z_i)$ becomes $0$\n",
    "\n",
    "* If $Z_i > 0$ and $Z_i \\rightarrow +\\infty \\ (\\forall i)$ then $\\exp(-Z_i) \\rightarrow 0$, therefore $\\log(1 + \\exp(-Z_i)) \\rightarrow 0$ (explanation can be found from the above graph)\n",
    "\n",
    "---\n",
    "\n",
    "To avoid the problems of overfitting and underfitting, we apply regularization techniques, i.e., we add $\\lambda$\n",
    "\n",
    "$$(2) \\implies w^* = argmin \\bigg[ \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) + \\lambda w^Tw \\bigg] \\rightarrow (3)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$(2) \\implies w^* = argmin \\bigg[ \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) + \\lambda ||w||_2^2 \\bigg] \\rightarrow (3)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$(2) \\implies w^* = argmin \\bigg[ \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) + \\lambda \\sum_{j=1}^d w_j^2 \\bigg] \\rightarrow (3)$$\n",
    "\n",
    "In $(3)$, first term is called the **loss term** and second term is called the **regularization term**. Here, the term $\\lambda$ is a hyperparameter.\n",
    "\n",
    "> $\\lambda = 0 \\implies$ overfitting <br> $\\lambda = \\text{very large} \\implies$ underfitting\n",
    "\n",
    "To summarize, the general pattern that is followed in machine learning is\n",
    "\n",
    "$$\\text{min}\\bigg(\\text{loss function over training data} + \\text{regularization}\\bigg)$$\n",
    "\n",
    "We find the right $\\lambda$ through cross validation techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L1$ Regularization and Sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using $L2$ norm (above), are there any viable regularization? Yes. We have $L1$ norm, when substituted in $(3)$, we get -\n",
    "\n",
    "$$(3) \\implies w^* = argmin \\bigg[ \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) + \\lambda ||w||\\bigg] \\rightarrow (4)$$\n",
    "\n",
    "or\n",
    "\n",
    "$$(3) \\implies w^* = argmin \\bigg[ \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) + \\lambda \\sum_{j=1}^d|w_j| \\bigg] \\rightarrow (4)$$\n",
    "\n",
    "$L1$ regulization serves the same purpose of $L2$ regularization but has other advantages (sparsity).\n",
    "\n",
    "* The solution to the logistic regression is said to be sparse iff it consists of many $0$'s.\n",
    "* All the unimportant features become $0$.\n",
    "\n",
    "On the other hand, we have ElasticNet which combines the advantages of both $L2$ and $L1$ norms. The equation of the same looks like -\n",
    "\n",
    "$$w^* = argmin \\bigg[ \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) + \\lambda_1 ||w|| + \\lambda_2 ||w||_2^2 \\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic interpretation for Logistic Regression\n",
    "\n",
    "Refer to → https://www.cs.cmu.edu/~tom/mlbook/NBayesLogReg.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* In other ways, we can derive logistic regression by probabilistic methods. It is the combination of **Gaussian Naive Bayes** and **Bernaulli Distribution**.\n",
    "\n",
    "* In the case of $y_i \\in \\{+1, -1\\}$ the optimization problem will be\n",
    "\n",
    "$$w^* = argmin \\bigg[ \\sum_{i=1}^n \\log \\big(1 + \\exp(-Z_i)\\big) + \\text{Regularization} \\bigg]$$\n",
    "\n",
    "* In the case of $y_i \\in \\{1, 0\\}$ the optimization problem will be probabilistic in nature\n",
    "\n",
    "$$w^* = argmin \\bigg[ \\sum_{i=1}^n -y_i \\log(p_i) - (1 - y_i) \\log(1 - p_i) + \\text{Regularization} \\bigg] \\ \\text{where} \\ p_i = \\sigma(w^Tx_i)$$\n",
    "\n",
    "Both are same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss minimization interpretation for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The aim of any optimization problem is to minimize the loss.\n",
    "\n",
    "* Let's imagine that we are appending \n",
    "\n",
    "> `+1` → incorrect classification <br> `+0` → correct classification\n",
    "\n",
    "* In this case, the aim should be to minimize the number of incorrectly classified points.\n",
    "* Thus, we have to find the parameter $w^*$ that minimizes error.\n",
    "* This type of loss function is called `0_1_loss_function`.\n",
    "\n",
    "From this,\n",
    "\n",
    "* The ideal loss function looks like -\n",
    "\n",
    "$$w^* = argmin \\sum_{i=1}^n \\textbf{0_1_loss_function}(y_i, x_i, w)$$\n",
    "\n",
    "**Different loss functions plot**\n",
    "\n",
    "<img src=\"http://i.stack.imgur.com/4DFDU.png\">\n",
    "\n",
    "**Credits** - Image from Internet\n",
    "\n",
    "**Note**: The optimization problems in machine learning is solved using the mathematical concepts like differentiation in calculus. If the optimization problem is not differentiable, then we cannot do much about it. In order to make it differentiable ready, we do approximation by applying logistic loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance and Model interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's say we have features like $(f_1, f_2, \\dots, f_j, \\dots, f_d)$ and corresponding to these features we have weights like $(w_1, w_2, \\dots, w_j, \\dots, w_d)$.\n",
    "\n",
    "* Let's also assume that all **features are independent** (Naive Bayes). Now, to determine which feature is important, we can simply rely on the absolute values of corresponding weights and pick the one which has more weight.\n",
    "    * If $w_j$ is large (irrespetive of the sign), the impact of $w_j$ to determine the class label increases and thus the corresponding $f_j$ is important for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collinearity of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If two features that can be represented in a linear fashion is called **Collinearity**.\n",
    "\n",
    "$$f_1 = \\alpha_1 + \\alpha_2 f_2$$\n",
    "\n",
    "* If more features are represented in a linear fashion - is called **multicollinearity**.\n",
    "\n",
    "$$f_1 = \\alpha_1 + \\alpha_2 f_2 + \\alpha_3 f_3 + \\dots + \\alpha_n f_n$$\n",
    "\n",
    "Let, $w^* = \\{1, 2, 3\\}$ are the weghts corresponding to features $\\{f_1, f_2, f_3\\}$. Let $x_q$ be $\\{x_{q1}, x_{q2}, x_{q3}\\}$\n",
    "\n",
    "Now, as per the model rule we do\n",
    "\n",
    "$\\implies w^Tx_q = w_1x_{q1} + w_2x_{q2} + w_3x_{q3}$\n",
    "\n",
    "$\\implies w^Tx_q = x_{q1} + 2x_{q2} + 3x_{q3}$\n",
    "\n",
    "Let $f_2 = 1.5f_1$\n",
    "\n",
    "$\\implies w^Tx_q = x_{q1} + 3x_{q1} + 3x_{q3}$\n",
    "\n",
    "$\\implies w^Tx_q = 4x_{q1} + 3x_{q3}$\n",
    "\n",
    "Therefore, the final weights are $\\{4, 0, 3\\}$. This simply concludes that $f_2$ is not important (which is wrong).\n",
    "\n",
    "**Note**: If features are collinear, then we cannot use |w_j| for the feature importance.\n",
    "\n",
    "**How do you determine if the features are multicollinear?**\n",
    "\n",
    "* Standardize the features\n",
    "* Compute the weights by applying optimization problem\n",
    "* Pertubate the features by adding slight noise\n",
    "* Recompute the weights by appling optimization problem\n",
    "* If initial weights and final weight differ significantly, then the features are said to be multicollinear (we cannot use $|w_j|$ to determine best features). Otherwise no."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression - Imbalanced data\n",
    "\n",
    "* Refer to this video - https://www.youtube.com/watch?v=l8Dge0z1Zks&ab_channel=AppliedAICourse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training - Time complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training in logistic regression is basically solving the given optimization problem. This process is called **stochastic gradient descent**.\n",
    "* The time complexity in the **training process** is roughly $O(nd)$ where $n$ is the total number of points and $d$ is the dimensionality.\n",
    "* The space complexity at runtime is $O(d)$ because the only thing that is required to store is $w^*$ which is a vector of optimized weights of $d$ dimensional space.\n",
    "* The time complexity is also $O(d)$ which means that we have $d$ features.\n",
    "---\n",
    "* If $d$ is small, the algorithm works like magic.\n",
    "* If $d$ is large, we can use $L1$ regularization which creates sparsity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real world cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The decision surface is linear in nature. It can be a line or plane or hyperplane.\n",
    "* The basic assumption here is that the data is linearly separated or almost linearly separated.\n",
    "* The impact of outliers is very less because of the sigmoid function. However, if outliers are problematic, then one can remove it and re-train the model to fit the data better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature engineering and feature transformation are the important aspects in solving machine learning problems.\n",
    "\n",
    "* In the case of non-linearly separable data, we have to do feature transformation in order to find the best separator.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1406/1*OpPID41jkJ70dslLHdP0_g.jpeg\">\n",
    "\n",
    "**Credits** - Image from Internet\n",
    "\n",
    "**Types of transformations**\n",
    "\n",
    "* Product transformation\n",
    "* Sqaure transformation\n",
    "* Trignometric transformation\n",
    "* Boolean transformation\n",
    "* Logarthmic transformation\n",
    "* Exponential transformation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
