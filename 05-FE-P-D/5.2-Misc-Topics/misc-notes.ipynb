{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If given a model $f$ which takes $x_q$ as input and returns $y_q$ - may or may not be the actual probability $P(y_q = 1 | x_q)$\n",
    "\n",
    "* We already have a trained model i.e., $f$.\n",
    "\n",
    "* To get the exact $P(y_q = 1 | x_q)$ and $P(y_q = 0 | x_q)$, we have follow a method called calibration.\n",
    "\n",
    "* Why we need this because - in ML given 2 class classification problem, we extensively use the log-loss metric.\n",
    "\n",
    "$$\\text{log-loss} = -\\frac{1}{n} \\sum_{i=1}^n \\big[y_i \\log{(p_i)} + (1 - y_i)\\log{(1 - p_i)}\\big]$$\n",
    "\n",
    "* We have to take the output of the model and on top of it, we have to build calibration model.\n",
    "    - This gives us the exact $P(y_q = 1 | x_q)$.\n",
    "\n",
    "> Predicting the probabilities allows some flexibility including -\n",
    "> * deciding how to interpret the probabilities.\n",
    "> * presenting predictions with uncertainty.\n",
    "> * providing more nuanced ways to evaluate the skill of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedure to Visualize Calibration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps**\n",
    "\n",
    "![calibration-plot](https://user-images.githubusercontent.com/63333753/126610858-11e971b5-1f20-45ff-91eb-31718cf1095b.PNG)\n",
    "\n",
    "* For a 2 class classification, fit a model to the training data, let that be $f$.\n",
    "    - $y_i$ is either 0 or 1.\n",
    "\n",
    "* Take the cross-validation data and predict $y_i$.\n",
    "    - Let the prediction value be $\\hat{y_i}$.\n",
    "    - Let the actual value be $y_i$.\n",
    "\n",
    "* Construct a table $\\{x_{q_i}, y_i, \\hat{y_i}\\}$.\n",
    "    - $x_{q_i} \\in$ cross-validation data.\n",
    "\n",
    "* Sort the table in the increasing or ascending order of $\\hat{y_i}$.\n",
    "\n",
    "* Chunkenize the sorted table into chunks of size `m`.\n",
    "\n",
    "* Calculate or compute the average of $y_i$, $\\hat{y_i}$, and $x_{q_i}$.\n",
    "\n",
    "* Consider the mean data of $y_i$ and $\\hat{y_i}$ from each chunk. This data is known as calibration data.\n",
    "     - The data set consists of $\\{\\hat{y}^{(i)}, y^{(i)}\\}$.\n",
    "\n",
    "* Now, the $P(y_q = 1 | x_q)$ are available in the mean data of $y_i$ from each chunk (because, the data is binary classification type 1 or 0).\n",
    "\n",
    "* Plot $\\{\\hat{y}^{(i)}, y^{(i)}\\}$.\n",
    "    - x-axis → $\\hat{y}^{(i)}$\n",
    "    - y-axis → $y^{(i)}$\n",
    "\n",
    "* We can also map the calibration data in a linear regression model and draw the best fit line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Platt’s Calibration / Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Given the calibration data $\\{\\hat{y}^{(i)}, y^{(i)}\\}$, we need to predict $y^{(i)}$ considering $\\hat{y}^{(i)}$.\n",
    "\n",
    "* The calibration plot looks more like a sigmoidal function. So, Platt has come up with an idea to use the modified form of sigmoid function such as -\n",
    "    - $\\hat{y}^{(i)}$ is coming from $[f(x_q) = y_q]$\n",
    "    - $y^{(i)}$ is coming from the actual $y_i$'s and it is $P(y_q = 1 | x_q)$\n",
    "    - Scaling equation\n",
    "    \n",
    "    $$\\implies P(y_q = 1 | x_q) = \\frac{1}{1 + \\exp{[A f(x_q) + B]}}$$\n",
    "    \n",
    "    - $A$ and $B$ are found by solving an optimization problem.\n",
    "\n",
    "* Platt scaling works iff the calibration data looks like sigmoidal function. Otherwise, in such cases, we opt the method od Isotonic calibration or Isotonic regression.\n",
    "\n",
    "* Isotonic calibration is the most used technique in the real world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isotonic Regression / Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When we have calibration data not in the form sigmoidal shape, we choose isotonic regression to predict the $y^{(i)}$.\n",
    "\n",
    "* Isotonic regression uses a method known piece-wise linear models that breaks down model building into multile sets of data (for ever step function) and tries to get the best approximation.\n",
    "\n",
    "![isotonic-piece-wise](https://user-images.githubusercontent.com/63333753/126634652-d63093b8-0abe-467c-bf40-e6f2d6b504e5.png)\n",
    "\n",
    "* The only problem is how do we know till which points the model $L_i$ should consider. For example, if there is a point which lies in the range of (7, 10), for predicting the $y^{(i)}$, we consider the model $L_3$. The ultimate question here is how do we find the exact threshold to decide which model to take which set of points.\n",
    "\n",
    "* To find so, we have to be solving an optimization problem (minimizing the errors between the point and the line). This helps to decide the threshold values.\n",
    "\n",
    "    - The optimization problem helps to find the thresholds along with find the slopes and intercepts for lines by putting a constraint that there should not be too many lines.\n",
    "    \n",
    "    - This optimization problem looks like an extended linear regression problem.\n",
    "\n",
    "* In isotonic regression we have more parameters to estimate than in Platt's scaling technique. Thus, we need more data or more points (calibration data).\n",
    "\n",
    "* It is suggested to use Platt's scaling when the calibration data is less than proceeding to use Isotonic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:red\">RANSAC</span> (<span style=\"color:red\">RAN</span>dom <span style=\"color:red\">SA</span>mpling <span style=\"color:red\">C</span>onsensus)</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is a statistical concept that focuses on the subject of building a machine learning model in the presence of outliers in the data.\n",
    "    - How to build a robust model in the presence of outliers?\n",
    "\n",
    "* This concept or technique is mostly used in the field of image processing and computer vision.\n",
    "\n",
    "* RANSAC is a model independent technique\n",
    "\n",
    "* Let's say we are building a linear regression model without using RANSAC. Now, in the case of outliers that are in the data, they tend to pull the best fit line towards them.\n",
    "\n",
    "![without_ransac](https://user-images.githubusercontent.com/63333753/126858877-f57c4286-0463-40d1-8cdc-4a0453889ff1.png)\n",
    "\n",
    "* **RANSAC model steps**\n",
    "    - From the whole training data ($D_{Train}$), take a random sample ($D_0$).\n",
    "        - Build a model on $D_0$ and let $L_0$ be a model for the best fit line.\n",
    "            - Most of the points in $D_0$ are going to be inliers and very few will be outliers.\n",
    "            - The line $L_0$ is less impacted with few outliers.\n",
    "    - Create a outlier set $O_0$ from $D_{Train}$ and construct another data set such as $D_{Train}^1 = D_{Train} - O_0$.\n",
    "        - Take a random sample ($D_1$) from $D_{Train}^1$.\n",
    "        - Build a model on $D_1$ and let $L_1$ be a model for the best fit line.\n",
    "    - Create a outlier set $O_1$ from $D_{Train}^1$ and construct another data set such as $D_{Train}^2 = D_{Train}^1 - O_1$.\n",
    "        - ...\n",
    "        - ...\n",
    "    - Repeat this whole process $k$ times until there is no difference between the models $L_i$ and $L_{i+1}$\n",
    "    - Final robust model ($L_*$) will be built on $D_{Train}^{i+1}$ where all the outliers will be removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraining Models Periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Whenever the data is collected on the daily basis, we need to re-train the model so as to perform well.\n",
    "\n",
    "**How do we know when to re-train the model?**\n",
    "\n",
    "* We need to re-train when the model's performance is dropping.\n",
    "* If the data set itself is changing (non-stationary) constantly, then model re-training should be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A/B Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It is one of the most important parts of machine learning. It is as important as modelling itself.\n",
    "\n",
    "* It is also known as bucket testing or split run or controlled experiments.\n",
    "\n",
    "* It is a user experience research methodology - randomized experiment with two variants, **A** and **B**.\n",
    "    - It includes application of **statistical hypothesis testing** or **two-sample hypothesis testing** as used in the field of statistics.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1838/1*aPUSwnuyUsxF_ACHuxiDlA.png\">\n",
    "\n",
    "* A/B testing method - is actually used to test it production environment.\n",
    "\n",
    "* We use the concept of **Confidence Interval** to see the performances of two models **A** and **B**.\n",
    "    - If both the models' CI's are overlapping them we should use old model.\n",
    "    - Otherwise, we can use new model.\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Science / ML Project Life - Cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Understand the business requirement.\n",
    "    - Defining the problems and objectives.\n",
    "\n",
    "2. Data Acquisition\n",
    "    - ETL\n",
    "    - SQL\n",
    "\n",
    "3. Data Preparation\n",
    "    - Cleaning\n",
    "    - Preprocessing\n",
    "\n",
    "4. Exploratory Data Analysis\n",
    "    - Data Analysis\n",
    "    - Data Visualization\n",
    "    - Hypothesis Testing\n",
    "\n",
    "5. Modelling, Evaluation & Interpretation\n",
    "\n",
    "6. Produce analytical reports\n",
    "    - Communication\n",
    "    - 1-pager reports\n",
    "\n",
    "7. Deployment\n",
    "    - Engineering\n",
    "\n",
    "8. Real world testing\n",
    "    - A/B testing\n",
    "\n",
    "9. Customer / Business buy-in\n",
    "\n",
    "10. Operations\n",
    "    - Retrain models\n",
    "    - Handle failure processes\n",
    "\n",
    "11. Optimization\n",
    "    - Improve models\n",
    "    - More data\n",
    "    - More features\n",
    "    - More optimization of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Productionization and Deployment of ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of productionizing the models -\n",
    "\n",
    "1. Using `sklearn` itself to store the model in hard disk and loading it in the `RAM` whenever we want to predict for the real (actual) value.\n",
    "    - We can use `pickle` file to store the model.\n",
    "\n",
    "2. Custom Implementation Approach\n",
    "    - a\n",
    "        - Store the parameters in a file or anything.\n",
    "        - During the runtime, while predicting the an input, implement the `predict()` function in `C` or `C++` or `JAVA`.\n",
    "        - Thus, we will have a low latency system.\n",
    "    - b\n",
    "        - If we are using logistic regression model, storing the coefficients in hashtable is better becuase hashtables (dictionaries) are way faster than other data structures.\n",
    "    - c\n",
    "        - If we are using a decision tree model, once we get the if else conditions, we have to implement the same in `C` or `C++` or `JAVA` to get the results faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
