{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommender System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A system that recommend items (movies, songs, books) based on what the user has searched or watched for - based on historical data of a user.\n",
    "\n",
    "    - Netflix movie recommendation\n",
    "    - YouTube movie recommendation\n",
    "    - ...\n",
    "\n",
    "* User is represented as $u_i$.\n",
    "\n",
    "* Item is represented as $I_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Formulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A dataset is like a matrix where each row corresponds to user ($u_i$) and each column corresponds to an item ($I_k$).\n",
    "    - The dimension of the matrix is $(N * M)$.\n",
    "\n",
    "* The values in the dataset are typically ratings or just boolean values.\n",
    "\n",
    "**Properties of a dataset (matrix) → $A$**\n",
    "\n",
    "* Matrix $A$ of ratings is very sparse\n",
    "    - $n = 1 \\text{million}$\n",
    "    - $m = 10k$\n",
    "    - $n * m = 10^{10}$\n",
    "    - There can be several empty values (sparsity)\n",
    "\n",
    "* Sparsity of $A$ is given as $\\frac{\\text{number of empty cells}}{\\text{total number of cells}}$\n",
    "\n",
    "> Recommender systems can be posed as a classification problem or a regression problem.\n",
    "\n",
    "* $u_i$ and $I_j$ combinely known as features. Featurization is a must step for the whole process.\n",
    "\n",
    "**Matrix Completion**\n",
    "\n",
    "* Fill up the empty cells with reasonable values based on the values in the non-empty cells. This is the problem of matrix completion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of RS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborative Filtering (CF)**\n",
    "\n",
    "* CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating).\n",
    "\n",
    "* Core idea → users who agreed in the past tend to also agree in the future.\n",
    "\n",
    "**Content-based Filtering**\n",
    "\n",
    "* Content-based filtering uses item features to recommend other items similar to what the user likes, based on their previous actions or explicit feedback.\n",
    "\n",
    "* It doesn't use the rating info for recommendation. Rather, it uses genre information (the category that an item belongs).\n",
    "\n",
    "<img src=\"https://i.pinimg.com/originals/2e/dc/ac/2edcac47fa446af6c8a7a5c2619dbc5e.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity-based Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**User-User Similarity**\n",
    "\n",
    "* Given a dataset matrix, each row corresponds to a user $(u_i)$. To find the similarity between $u_i$ and $u_j$, we can use cosine-similarity concept.\n",
    "\n",
    "$$\\text{sim}(u_i, u_j) = \\text{cosine}(u_i, u_j) = \\frac{u_i^T u_j}{||u_i||*||u_j||}$$\n",
    "\n",
    "* If user's preferences change over time, then the recommendations may not so accurate.\n",
    "\n",
    "**Item-Item Similarity**\n",
    "\n",
    "* Given a dataset matrix, each row corresponds to a user $(u_i)$. To find the similarity between $u_i$ and $u_j$, we can use cosine-similarity concept.\n",
    "\n",
    "$$\\text{sim}(I_k, I_l) = \\text{cosine}(I_k, I_l) = \\frac{I_k^T I_l}{||I_k||*||I_l||}$$\n",
    "\n",
    "* Rating's on a given item do not change significantly over time after the initial period.\n",
    "\n",
    "<img src=\"https://media.springernature.com/lw685/springer-static/image/art%3A10.1007%2Fs11227-020-03266-2/MediaObjects/11227_2020_3266_Fig1_HTML.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization (MF)\n",
    "\n",
    "PCA & SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decomposing a matrix into product of two or more matrices is called Matrix Factorization (as long as it follows multiplicative decomposition). \n",
    "\n",
    "* PCA can be written as MF and is oftern referred to as eigen-decomposition of data matrix and often performed on the covariance matrix.\n",
    "\n",
    "* SVD is MF technique related to PCA that can be performed on any rectangular matrix.\n",
    "\n",
    "<img src=\"https://research.fb.com/wp-content/uploads/2016/11/post00049_image0001.png\">\n",
    "\n",
    "**Credits** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF / NNFM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a matrix $A_{n * m}$, we want to decompose $A$ into two matrices as -\n",
    "\n",
    "$$A_{n * m} = B_{n * d} (C^T_{d * m})$$\n",
    "\n",
    "such that $B_{ij} \\geq 0$ and $C_{ij} \\geq 0 \\ \\forall \\ i, j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization for CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let\n",
    "\n",
    "$$A = B * C^T$$\n",
    "\n",
    "Where\n",
    "\n",
    "* $A$ dimension is $n * m$\n",
    "* $B$ dimension is $n * d$\n",
    "* $C^T$ dimension is $d * m$\n",
    "\n",
    "Here $d > 0$ and $d \\leq \\text{min}(m, n)$. We can write $A_{ij}$ as -\n",
    "\n",
    "$$A_{ij} = B^T_{i} * C_j$$\n",
    "\n",
    "We can use the above as an optimization problem using $SGD$ and the values of $B$ and $C$ need to be found.\n",
    "\n",
    "$$\\text{argmin}_{B, C} = \\sum_{A_{ij} \\ \\text{is not empty}} \\big(A_{ij} - B^T_iC_j\\big) ^ 2 \\implies \\text{squared loss}$$\n",
    "\n",
    "Once we solve the above problem we get $B$ and $C$ matrices.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "* Solve optimization problem using SGD.\n",
    "\n",
    "* Compute matrix $B$ where each row is $B_i$ and matrix $C$ where each row is $C_j$.\n",
    "\n",
    "* Matrix completion $\\{\\hat{A} = B * C^T\\}$ such that $\\hat{A} - A$ is very minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF for Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w.k.t\n",
    "\n",
    "$$A = B * C^T$$\n",
    "\n",
    "Where\n",
    "\n",
    "* $A$ dimension is $n * m$\n",
    "* $B$ dimension is $n * d$\n",
    "* $C^T$ dimension is $d * m$\n",
    "\n",
    "Here $d > 0$ and $d \\leq \\text{min}(m, n)$.\n",
    "\n",
    "* Matrix $B$ consists of user data in each row and $B_i$ can be taken as user vector.\n",
    "    - if $u_i$ and $u_j$ are very similar, then $\\text{dist}(u_i, u_j)$ will be small.\n",
    "* Matrix $C$ consists of item data in each column and $C_i$ can be taken as item vector.\n",
    "    - if $I_i$ and $I_j$ are very similar, then $\\text{dist}(I_i, I_j)$ will be small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. For implementing item-item Recommender system(Collaboratory Filtering) scheme, we've a math concept called Matrix Factorization.\n",
    "\n",
    "* We know that\n",
    "    $$A = B * C^T$$\n",
    "\n",
    "    - Where\n",
    "\n",
    "        * $A$ dimension is $n * m$\n",
    "        * $B$ dimension is $n * d$\n",
    "        * $C^T$ dimension is $d * m$\n",
    "    \n",
    "    - The optimization problem here is\n",
    "    \n",
    "    $$\\text{argmin}_{B, C} = \\sum_{A_{ij} \\ \\text{is not empty}} \\big(A_{ij} - B^T_iC_j\\big) ^ 2 \\implies \\text{squared loss}$$\n",
    "\n",
    "3. The task at hand is to find $d$ for which error is low. Here $d$ is our hyper parameter. We need to try for various $d$ and plot error vs $d$ graph. The $d$ value for which error is $min$ is taken as the best $d$.\n",
    "\n",
    "4. At this $d$, now we have $B$ and $C$ matrices ready. $B * C^T$ would give $\\hat{A}$.\n",
    "\n",
    "5. That's all our recommender system predicted rating matrix is ready."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MF for RS\n",
    "\n",
    "Research paper → https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $r_{ui}$ → rating given by a user on item $i$\n",
    "    - can also be considered as $A_{ij}$\n",
    "\n",
    "* $q_i$ → item-vector\n",
    "    - can also be considered as $B_i$\n",
    "\n",
    "* $p_u$ → user-vector\n",
    "    - can also be considered as $C_j$\n",
    "\n",
    "* The optimization problem is \n",
    "    $$\\text{argmin}_{q, p} = \\sum_{r_{ij} \\ \\text{is not empty}} \\big(r_{ij} - q^T_ip_u\\big) ^ 2 + \\lambda \\big[||q_i||^2 + ||p_u||^2\\big] \\rightarrow (1)$$\n",
    "\n",
    "    * We can solve $(1)$ using SGD (takes more time)\n",
    "    * We can also using alternating least squares (ALS) - faster algorithm\n",
    "        - fix $p_u$ as constant and find $q_i$ using gradient descent\n",
    "        - fix $q_i$ as constant and find $p_u$ using gradient descent\n",
    "        - continue the above until the process is converged\n",
    "\n",
    "* We want to remove bias from the actual formulation, we will get the following equation\n",
    "    $$\\text{argmin}_{q, p, b} = \\sum_{r_{ij} \\ \\text{is not empty}} \\big(r_{ij} -\\mu - bu - bi - q^T_ip_u\\big) ^ 2 + \\lambda \\big[||q_i||^2 + ||p_u||^2 + bu^2 + bi^2\\big] \\rightarrow (2)$$\n",
    "    \n",
    "    - Where\n",
    "        - $\\mu$ → average rating across all the users and items\n",
    "        - $bu$ → user bias (positive or negative)\n",
    "        - $bi$ → item bias\n",
    "\n",
    "* For each user-vector, we will just sort this vector and pick the recommendations from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold Start Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When a new user comes, whole row will be empty. Similarly, when a new item comes, whole column will be empty. In  such cases the best and simple solution is to recommend the top items (Content-based RS).\n",
    "\n",
    "* For new users:\n",
    "    * These top items are basically decided based on the meta data like -\n",
    "        - geo-location\n",
    "        - browser\n",
    "        - device\n",
    "        - ...\n",
    "\n",
    "* For new items:\n",
    "    - These top (...) are basically decided based on the meta data like -\n",
    "        - category\n",
    "        - price\n",
    "        - description\n",
    "        - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W2V as MF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First build the co-occurrence matrix $(X)$ where $X_{ij}$ is an element in $X$. The element $X_{ij}$ signifies the number of times a word $w_j$ occurs in the context of the word $w_i$.\n",
    "    - $w_j$ occurs in the context of $w_i$ if $w_i$ and $w_j$ are within a neighborhood of some distance.\n",
    "\n",
    "    - the matrix $X$ is of size $(n * n)$ where $n$ is the number of words\n",
    "\n",
    "* Now, $X$ can be represented as $X_{n*n} = U_{n*n} \\sum_{n*n} V^T_{n*n}$. This is implemented using SVD techniques.\n",
    "    - Again, using the techniques of Truncated SVD(k) we will have $X_{n*n} = U_{n*k} \\sum_{k*k} V^T_{k*n}$ after truncating the matrices.\n",
    "\n",
    "<img src=\"https://research.fb.com/wp-content/uploads/2016/11/post00049_image0001.png\">\n",
    "\n",
    "**Credtis** - Image from Internet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen-Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eigen-Faces is a techniques based on PCA that is applied on images.\n",
    "    - Ideally used for face recognition.\n",
    "\n",
    "* **Procedure**\n",
    "\n",
    "    1. Construct a matrix $A$ where each image is a row vector.\n",
    "    2. Compute the PCA to get $W$ and $\\Lambda$ matrices.\n",
    "    3. Using $W$ compute $W_k$ which is simply considering the top $k$ eigen values and corresponding eigen vector.\n",
    "    4. Take orginal matrix $A$ and multiply with $W_k$ to get $\\hat{A}$ (Eigen-Faces)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key differences between PCA and SVD:-\n",
    "\n",
    "1. SVD uses randomized algorithm which is not used in PCA.\n",
    "2. SVD operates on sparse matrices but PCA doesn't.\n",
    "3. SVD is faster than PCA.\n",
    "4. More often we use SVD than PCA.\n",
    "5. SVD is a better dimensionality reduction technique than PCA, we sometimes use PCA and SVD interchangeably in discussion because both have same application , though SVD is more advanced.\n",
    "6. PCA needs covariance matrix to be applied on whereas we can directly input any rectangular matrix to SVD algo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
